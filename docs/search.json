[
  {
    "objectID": "Neurocomp.html",
    "href": "Neurocomp.html",
    "title": "Neurocomputational modeling",
    "section": "",
    "text": "Computational neuroscience is about explaining brain functioning at various levels (neural activity patterns, behavior, etc.) using biologically realistic neuro-computational models. Different types of neural and synaptic mathematical models are used in the field, abstracting biological complexity at different levels. There is no “right” level of biological plausibility for a model - you can always add more details -, but you have to find a paradigm that allows you to:"
  },
  {
    "objectID": "Neurocomp.html#neuron-models",
    "href": "Neurocomp.html#neuron-models",
    "title": "Neurocomputational modeling",
    "section": "Neuron models",
    "text": "Neuron models\nRate-coded neurons only represent the instantaneous firing rate of a neuron:\n\n    \\tau \\, \\frac{d v(t)}{dt} + v(t) = \\sum_{i=1}^d w_{i, j} \\, r_i(t) + b\n \n    r(t) = f(v(t))\n\n\n\n\n\n\nSpiking neurons emit binary spikes when their membrane potential exceeds a threshold. Some examples:\n\nLeaky integrate-and-fire (LIF):\n\n\n    C \\, \\frac{d v(t)}{dt} = - g_L \\, (v(t) - V_L) + I(t)\n \n    \\text{if} \\; v(t) > V_T \\; \\text{emit a spike and reset.}\n\n\n\n\n\n\n\nIzhikevich quadratic IF (Izhikevich, 2003):\n\n\\begin{cases}\n    \\displaystyle\\frac{dv}{dt} = 0.04 \\, v^2 + 5 \\, v + 140 - u + I \\\\\n    \\\\\n    \\displaystyle\\frac{du}{dt} = a \\, (b \\, v - u) \\\\\n\\end{cases}\n\n\nAdaptive exponential IF (AdEx, Brette and Gerstner, 2005).\n\n\n\\begin{cases}\n\\begin{aligned}\n    C \\, \\frac{dv}{dt} = -g_L \\ (v - E_L) + & g_L \\, \\Delta_T \\, \\exp(\\frac{v - v_T}{\\Delta_T}) \\\\\n                                            & + I - w\n\\end{aligned}\\\\\n\\\\\n    \\tau_w \\, \\displaystyle\\frac{dw}{dt} = a \\, (v - E_L) - w\\\\\n\\end{cases}\n\n\n\n\n\n\nBiological neurons do not all respond the same to an input current: Some fire regularly, some slow down with time., some emit bursts of spikes… Modern spiking neuron models allow to recreate these dynamics by changing just a few parameters.\n\n\n\n\n\nRecurrent neural networks (e.g. randomly connected populations of neurons) can exhibit very rich dynamics even in the absence of inputs:\n\nOscillations at the population level.\nExcitatory/inhibitory balance.\nSpatio-temporal separation of inputs (reservoir computing)."
  },
  {
    "objectID": "Neurocomp.html#synaptic-plasticity",
    "href": "Neurocomp.html#synaptic-plasticity",
    "title": "Neurocomputational modeling",
    "section": "Synaptic plasticity",
    "text": "Synaptic plasticity\n\nHebbian plasticity\nHebbian learning postulates that synapses strengthen based on the correlation between the activity of the pre- and post-synaptic neurons:\n\nWhen an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A’s efficiency, as one of the cells firing B, is increased.\nDonald Hebb, 1949\n\nIn Hebbian learning, the weights increase proportionally to the the product of the pre- and post-synaptic firing rates:\n\\frac{dw}{dt} = \\eta \\, r^\\text{pre} \\, r^\\text{post}\nThe BCM (Bienenstock et al., 1982; Intrator and Cooper, 1992) plasticity rule allows LTP and LTD depending on the post-synaptic plasticity:\n\\frac{dw}{dt} = \\eta \\, r^\\text{pre} \\, r^\\text{post}  \\,  (r^\\text{post} - \\mathbb{E}((r^\\text{post})^2))\nThe Covariance learning rule (Dayan and Abbott, 2001) compares the post-synaptic firing rate to a threshold:\n\\frac{dw}{dt} = \\eta \\, r^\\text{pre} \\, (r^\\text{post} - \\mathbb{E}(r^\\text{post}))\nThe Oja learning rule (Oja, 1982) allows to limit infinite weight growth:\n\\frac{dw}{dt}= \\eta \\, r^\\text{pre} \\, r^\\text{post} - \\alpha \\, (r^\\text{post})^2 \\, w\nWith rate-ocded networks, you can use virtually anything virtually any learning rule depending only on the pre- and post-synaptic firing rates, e.g. in Vitay and Hamker (2010):\n\\begin{aligned}\n    \\frac{dw}{dt}  & = \\eta \\, ( \\text{DA}(t) - \\overline{\\text{DA}}) \\, (r^\\text{post} - \\mathbb{E}(r^\\text{post}) )^+  \\, (r^\\text{pre} - \\mathbb{E}(r^\\text{pre}))- \\alpha(t) \\,  ((r^\\text{post} - \\mathbb{E}(r^\\text{post} )^+ )^2  \\, w\n\\end{aligned}\n\n\n\nSTDP: Spike-timing dependent plasticity\nSynaptic efficiencies actually evolve depending on the the causation between the neuron’s firing patterns:\n\nIf the pre-synaptic neuron fires before the post-synaptic one, the weight is increased (long-term potentiation). Pre causes Post to fire.\nIf it fires after, the weight is decreased (long-term depression). Pre does not cause Post to fire.\n\nThe STDP (spike-timing dependent plasticity, Bi and Poo, 2001) plasticity rule describes how the weight of a synapse evolves when the pre-synaptic neuron fires at t_\\text{pre} and the post-synaptic one fires at t_\\text{post}.\n \\frac{dw}{dt} = \\begin{cases} A^+ \\, \\exp - \\frac{t_\\text{pre} - t_\\text{post}}{\\tau^+} \\; \\text{if} \\; t_\\text{post} > t_\\text{pre}\\\\  \n    \\\\\n    A^- \\, \\exp - \\frac{t_\\text{pre} - t_\\text{post}}{\\tau^-} \\; \\text{if} \\; t_\\text{pre} > t_\\text{post}\\\\ \\end{cases}\n\nSTDP can be implemented online using traces. More complex variants of STDP (triplet STDP) exist, but this is the main model of synaptic plasticity in spiking networks.\n\n\n\nSource: Bi and Poo (2001)"
  },
  {
    "objectID": "Neurocomp.html#neuro-computational-models",
    "href": "Neurocomp.html#neuro-computational-models",
    "title": "Neurocomputational modeling",
    "section": "Neuro-computational models",
    "text": "Neuro-computational models\nPopulations of neurons can be combined in functional neuro-computational models learning to solve various tasks. They require to implement one (or more) equations per neuron and synapse (thousands of neurons, millions of synapses..).\nHere are some recent examples for our lab:\n\n\nBasal Ganglia\n\n\n\nSource: Villagrasa et al. (2018)\n\n\n\nHippocampus\n\n\n\nSource: Gönner et al. (2017)\n\n\n\nDopaminergic system\n\n\n\nSource: Vitay and Hamker (2014)"
  },
  {
    "objectID": "ANNarchy.html",
    "href": "ANNarchy.html",
    "title": "ANNarchy",
    "section": "",
    "text": "There already exist several neuro-simulators for computational neuroscience, each which different levels of description (detailed, spiking, rate-coded, etc.) or supported hardware (CPU, GPU).\nSome simulators provide fixed libraries of neural and synaptic models:\n\nNEURON https://neuron.yale.edu/neuron/\n\nMulti-compartmental models, spiking neurons (CPU)\n\nGENESIS http://genesis-sim.org/\n\nMulti-compartmental models, spiking neurons (CPU) –>\n\nNEST https://nest-initiative.org/\n\nSpiking neurons (CPU)\n\nGeNN https://genn-team.github.io/genn/\n\nSpiking neurons (GPU)\n\nAuryn https://fzenke.net/auryn/doku.php\n\nSpiking neurons (CPU)\n\n\nSome rely instead on code generation:\n\nBrian https://briansimulator.org/\n\nSpiking neurons (CPU)\n\nBrian2CUDA https://github.com/brian-team/brian2cuda\n\nSpiking neurons (GPU)\n\nANNarchy https://github.com/ANNarchy/ANNarchy\n\nRate-coded and spiking neurons (CPU, GPU)"
  },
  {
    "objectID": "ANNarchy.html#annarchy-artificial-neural-networks-architect",
    "href": "ANNarchy.html#annarchy-artificial-neural-networks-architect",
    "title": "ANNarchy",
    "section": "ANNarchy (Artificial Neural Networks architect)",
    "text": "ANNarchy (Artificial Neural Networks architect)\n\nResources\nWhite paper:\nVitay et al. (2015)\nANNarchy: a code generation approach to neural simulations on parallel hardware.\nFrontiers in Neuroinformatics 9. doi:10.3389/fninf.2015.00019\nSource code:\nhttps://github.com/ANNarchy/ANNarchy\nDocumentation:\nhttps://annarchy.github.io\nForum:\nhttps://groups.google.com/forum/#!forum/annarchy\n\n\nInstallation\nInstallation guide: https://annarchy.github.io/Installation/\nUsing pip:\npip install ANNarchy\nFrom source:\ngit clone https://github.com/ANNarchy/ANNarchy.git\ncd annarchy\npip install -e .\nRequirements (Linux and MacOS):\n\ng++/clang++\npython >= 3.6\nnumpy\nsympy\ncython\n\n\n\nFeatures\n\nSimulation of both rate-coded and spiking neural networks.\nOnly local biologically realistic mechanisms are possible (no backpropagation).\nEquation-oriented description of neural/synaptic dynamics (à la Brian).\nCode generation in C++, parallelized using OpenMP on CPU and CUDA on GPU (MPI is coming).\nSynaptic, intrinsic and structural plasticity mechanisms.\n\n\n\nStructure of a script\nA neuro-computational model in ANNarchy is composed of:\n\nSeveral populations implementing different neuron models.\nSeveral projections between the populations, that can implement specific synapse models.\nMonitors to record what is happening during a simulation.\n\n\n\n\n\n\nThe following script provides the basic structure of a model. First, the neuron and synapse models have to be defined using the equation-oriented interface. Populations are then created and connected with each other using projections. The network can then be generated and compiled, before the simulation can start.\nfrom ANNarchy import *\n\n# Create neuron types\nneuron = Neuron(...) \n\n# Create synapse types for transmission and/or plasticity\nstdp = Synapse(...) \n\n# Create populations of neurons\npop = Population(1000, neuron) \n\n# Connect the populations through projections\nproj = Projection(pop, pop, 'exc', stdp) \nproj.connect_fixed_probability(weights=Uniform(0.0, 1.0), probability=0.1)\n\n# Generate and compile the code\ncompile() \n\n# Record spiking activity\nm = Monitor(pop, ['spike']) \n\n# Simulate for 1 second\nsimulate(1000.)\nThe rest of this tutorial explains step by step how to implement those different mechanisms, starting with rate-coded networks, followed by spiking networks."
  },
  {
    "objectID": "Spiking.html",
    "href": "Spiking.html",
    "title": "Spiking networks",
    "section": "",
    "text": "Spiking neurons must also define two additional fields:\n\nspike: condition for emitting a spike.\nreset: what happens after a spike is emitted (at the start of the refractory period).\n\nA refractory period in ms can also be specified.\n\n\n\n\n\nExample of the Leaky Integrate-and-Fire:\n\n    C \\, \\frac{d v(t)}{dt} = - g_L \\, (v(t) - V_L) + I(t)\n\n\n    \\text{if} \\; v(t) > V_T \\; \\text{emit a spike and reset.}\n\nLIF = Neuron(\n    parameters = \"\"\"\n        C = 200.\n        g_L = 10.\n        E_L = -70.\n        v_T = 0.\n        v_r = -58.\n        I = 0.25\n    \"\"\",\n    equations = \"\"\"\n        C * dv/dt = g_L * (E_L - v) + I : init=E_L     \n    \"\"\",\n    spike = \"v >= v_T\",\n    reset = \"v = v_r\",\n    refractory = 2.0\n)\n\n\n\n\n\n\nNotebook: AdEx neuron - Adaptive exponential Integrate-and-fire\n\n\n\nDownload the Jupyter notebook: AdEx.ipynb\nRun it directly on colab: AdEx.ipynb"
  },
  {
    "objectID": "Spiking.html#conductances-currents",
    "href": "Spiking.html#conductances-currents",
    "title": "Spiking networks",
    "section": "Conductances / currents",
    "text": "Conductances / currents\nA pre-synaptic spike arriving to a spiking neuron increases the conductance/current g_target (e.g. g_exc or g_inh, depending on the projection).\nLIF = Neuron(\n    parameters = \"...\",\n    equations = \"\"\"\n        C*dv/dt = g_L*(E_L - v) + g_exc : init=E_L    \n    \"\"\",\n    spike = \"v >= v_T\",\n    reset = \"v = v_r\",\n    refractory = 2.0\n)\nEach spike increments instantaneously g_target from the synaptic efficiency w of the corresponding synapse.\ng_target += w\n\n\n\n\n\nFor exponentially-decreasing or alpha-shaped synapses, ODEs have to be introduced for the conductance/current.\nThe exponential numerical method should be preferred, as integration is exact.\nLIF = Neuron(\n    parameters = \"...\",\n    equations = \"\"\"\n        C*dv/dt = g_L*(E_L - v) + g_exc : init=E_L   \n\n        tau_exc * dg_exc/dt = - g_exc : exponential\n    \"\"\",\n    spike = \"v >= v_T\",\n    reset = \"v = v_r\",\n    refractory = 2.0\n)\n\n\n\n\n\n\nNotebook: Synaptic transmission\n\n\n\nDownload the Jupyter notebook: SynapticTransmission.ipynb\nRun it directly on colab: SynapticTransmission.ipynb\n\n\n\n\n\n\n\n\nNotebook: COBA - Conductance-based E/I network\n\n\n\nDownload the Jupyter notebook: COBA.ipynb\nRun it directly on colab: COBA.ipynb"
  },
  {
    "objectID": "Spiking.html#spike-timing-dependent-plasticity-stdp",
    "href": "Spiking.html#spike-timing-dependent-plasticity-stdp",
    "title": "Spiking networks",
    "section": "Spike-Timing Dependent plasticity (STDP)",
    "text": "Spike-Timing Dependent plasticity (STDP)\npost_spike similarly defines what happens when a post-synaptic spike is emitted.\nSTDP = Synapse(\n    parameters = \"\"\"\n        tau_plus = 20.0 : projection ; tau_minus = 20.0 : projection\n        A_plus = 0.01 : projection   ; A_minus = 0.01 : projection\n        w_min = 0.0 : projection     ; w_max = 1.0 : projection\n    \"\"\",\n    equations = \"\"\"\n        tau_plus  * dx/dt = -x : event-driven # pre-synaptic trace\n        tau_minus * dy/dt = -y : event-driven # post-synaptic trace\n    \"\"\",\n    pre_spike=\"\"\"\n        g_target += w\n        x += A_plus * w_max\n        w = clip(w + y, w_min , w_max)\n    \"\"\",\n    post_spike=\"\"\"\n        y -= A_minus * w_max\n        w = clip(w + x, w_min , w_max)\n    \"\"\")\n\n\n\n\n\n\nNotebook: STDP\n\n\n\nDownload the Jupyter notebook: STDP.ipynb\nRun it directly on colab: STDP.ipynb"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ANNarchy - Artificial Neural Networks architect",
    "section": "",
    "text": "ANNarchy (Artificial Neural Networks architect) is a neuro-simulator for rate-coded or spiking neural networks in Python. The source code is available at https://github.com/ANNarchy/ANNarchy, while the full documentation is at: https://annarchy.github.io.\nThis website is a tutorial for beginners to get started: for more details and functionalities, please refer to the documentation. The slides used for the videos are available here. Jupyter notebooks for simple models showcasing the usage of ANNarchy are available here."
  },
  {
    "objectID": "slides/presentation.html",
    "href": "slides/presentation.html",
    "title": "ANNarchy",
    "section": "",
    "text": "Outline\n\nNeurocomputational models\nNeuro-simulator ANNarchy\nRate-coded networks\nSpiking networks\n\n\n\n1 - Neurocomputational models\n\n\nComputational neuroscience\n\n\n\nComputational neuroscience is about explaining brain functioning at various levels (neural activity patterns, behavior, etc.) using biologically realistic neuro-computational models.\nDifferent types of neural and synaptic mathematical models are used in the field, abstracting biological complexity at different levels.\nThere is no “right” level of biological plausibility for a model - you can always add more details -, but you have to find a paradigm that allows you to:\n\n\nExplain the current experimental data.\nMake predictions that can be useful to experimentalists.\n\n\n\n\n\nSource: Kriegeskorte and Douglas (2018)\n\n\n\n\n\n\nRate-coded and spiking neurons\n\n\n\nRate-coded neurons only represent the instantaneous firing rate of a neuron:\n\n\n    \\tau \\, \\frac{d v(t)}{dt} + v(t) = \\sum_{i=1}^d w_{i, j} \\, r_i(t) + b\n\n\n    r(t) = f(v(t))\n\n\n\n\n\n\n\n\nSpiking neurons emit binary spikes when their membrane potential exceeds a threshold (leaky integrate-and-fire, LIF):\n\n\n    C \\, \\frac{d v(t)}{dt} = - g_L \\, (v(t) - V_L) + I(t)\n\n\n    \\text{if} \\; v(t) > V_T \\; \\text{emit a spike and reset.}\n\n\n\n\n\n\n\n\n\n\nSeveral spiking neuron models are possible\n\n\n\nIzhikevich quadratic IF (Izhikevich, 2003).\n\n\\begin{cases}\n    \\displaystyle\\frac{dv}{dt} = 0.04 \\, v^2 + 5 \\, v + 140 - u + I \\\\\n    \\\\\n    \\displaystyle\\frac{du}{dt} = a \\, (b \\, v - u) \\\\\n\\end{cases}\n\n\nAdaptive exponential IF (AdEx, Brette and Gerstner, 2005).\n\n\n\\begin{cases}\n\\begin{aligned}\n    C \\, \\frac{dv}{dt} = -g_L \\ (v - E_L) + & g_L \\, \\Delta_T \\, \\exp(\\frac{v - v_T}{\\Delta_T}) \\\\\n                                            & + I - w\n\\end{aligned}\\\\\n\\\\\n    \\tau_w \\, \\displaystyle\\frac{dw}{dt} = a \\, (v - E_L) - w\\\\\n\\end{cases}\n\n\n\n\n\n\n\n\n\nRealistic neuron models can reproduce a variety of dynamics\n\nBiological neurons do not all respond the same to an input current: Some fire regularly, some slow down with time., some emit bursts of spikes…\nModern spiking neuron models allow to recreate these dynamics by changing just a few parameters.\n\n\n\n\n\n\n\n\nPopulations of neurons\n\nRecurrent neural networks (e.g. randomly connected populations of neurons) can exhibit very rich dynamics even in the absence of inputs:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOscillations at the population level.\nExcitatory/inhibitory balance.\nSpatio-temporal separation of inputs (reservoir computing).\n\n\n\n\n\n\n\n\n\n\nSynaptic plasticity: Hebbian learning\n\nHebbian learning postulates that synapses strengthen based on the correlation between the activity of the pre- and post-synaptic neurons:\n\n\nWhen an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A’s efficiency, as one of the cells firing B, is increased.\nDonald Hebb, 1949\n\n\nWeights increase proportionally to the the product of the pre- and post-synaptic firing rates:\n\n\\frac{dw}{dt} = \\eta \\, r^\\text{pre} \\, r^\\text{post}\n\n\n\nSource: https://slideplayer.com/slide/11511675/\n\n\n\n\nSynaptic plasticity: Hebbian-based learning\n\nThe BCM (Bienenstock et al., 1982; Intrator and Cooper, 1992) plasticity rule allows LTP and LTD depending on the post-synaptic plasticity:\n\n\\frac{dw}{dt} = \\eta \\, r^\\text{pre} \\, r^\\text{post}  \\,  (r^\\text{post} - \\mathbb{E}((r^\\text{post})^2))\n\n\n\nCovariance learning rule (Dayan and Abbott, 2001):\n\n\\frac{dw}{dt} = \\eta \\, r^\\text{pre} \\, (r^\\text{post} - \\mathbb{E}(r^\\text{post}))\n\nOja learning rule (Oja, 1982):\n\n\\frac{dw}{dt}= \\eta \\, r^\\text{pre} \\, r^\\text{post} - \\alpha \\, (r^\\text{post})^2 \\, w\n\n\n\n\nSource: http://www.scholarpedia.org/article/BCM_theory\n\n\n\n\nor virtually anything depending only on the pre- and post-synaptic firing rates, e.g. Vitay and Hamker (2010):\n\\begin{aligned}\n    \\frac{dw}{dt}  & = \\eta \\, ( \\text{DA}(t) - \\overline{\\text{DA}}) \\, (r^\\text{post} - \\mathbb{E}(r^\\text{post}) )^+  \\, (r^\\text{pre} - \\mathbb{E}(r^\\text{pre}))- \\alpha(t) \\,  ((r^\\text{post} - \\mathbb{E}(r^\\text{post} )^+ )^2  \\, w\n\\end{aligned}\n\n\n\nSTDP: Spike-timing dependent plasticity\n\nSynaptic efficiencies actually evolve depending on the the causation between the neuron’s firing patterns:\n\nIf the pre-synaptic neuron fires before the post-synaptic one, the weight is increased (long-term potentiation). Pre causes Post to fire.\nIf it fires after, the weight is decreased (long-term depression). Pre does not cause Post to fire.\n\n\n\n\n\nSource: Bi and Poo (2001)\n\n\n\n\nSTDP: Spike-timing dependent plasticity\n\n\n\nThe STDP (spike-timing dependent plasticity, Bi and Poo, 2001) plasticity rule describes how the weight of a synapse evolves when the pre-synaptic neuron fires at t_\\text{pre} and the post-synaptic one fires at t_\\text{post}.\n\n \\frac{dw}{dt} = \\begin{cases} A^+ \\, \\exp - \\frac{t_\\text{pre} - t_\\text{post}}{\\tau^+} \\; \\text{if} \\; t_\\text{post} > t_\\text{pre}\\\\  \n    \\\\\n    A^- \\, \\exp - \\frac{t_\\text{pre} - t_\\text{post}}{\\tau^-} \\; \\text{if} \\; t_\\text{pre} > t_\\text{post}\\\\ \\end{cases}\n\n\nSTDP can be implemented online using traces.\nMore complex variants of STDP (triplet STDP) exist, but this is the main model of synaptic plasticity in spiking networks.\n\n\n\n\n\nSource: Bi and Poo (2001)\n\n\n\n\n\n\nNeuro-computational modeling\n\nPopulations of neurons can be combined in functional neuro-computational models learning to solve various tasks.\nNeed to implement one (or more) equations per neuron and synapse (thousands of neurons, millions of synapses..).\n\n\n\nBasal Ganglia\n\n\n\nSource: Villagrasa et al. (2018)\n\n\n\nHippocampus\n\n\n\nSource: Gönner et al. (2017)\n\n\n\nDopaminergic system\n\n\n\nSource: Vitay and Hamker (2014)\n\n\n\n\n\n\n2 - Neuro-simulator ANNarchy\n\n\nNeuro-simulators\n\n\nFixed libraries of models\n\nNEURON\nhttps://neuron.yale.edu/neuron/\n\nMulti-compartmental models, spiking neurons (CPU)\n\n\n\n\nNEST\nhttps://nest-initiative.org/\n\nSpiking neurons (CPU)\n\nGeNN\nhttps://genn-team.github.io/genn/\n\nSpiking neurons (GPU)\n\nAuryn\nhttps://fzenke.net/auryn/doku.php\n\nSpiking neurons (CPU)\n\n\n\nCode generation\n\nBrian\nhttps://briansimulator.org/\n\nSpiking neurons (CPU)\n\nBrian2CUDA\nhttps://github.com/brian-team/brian2cuda\n\nSpiking neurons (GPU)\n\nANNarchy\nhttps://github.com/ANNarchy/ANNarchy\n\nRate-coded and spiking neurons (CPU, GPU)\n\n\n\n\n\n\nANNarchy (Artificial Neural Networks architect)\n\nVitay et al. (2015)\nANNarchy: a code generation approach to neural simulations on parallel hardware.\nFrontiers in Neuroinformatics 9. doi:10.3389/fninf.2015.00019\n\n\n\n\n\n\n\n\n\n\nSource code:\n\nhttps://github.com/ANNarchy/ANNarchy\n\nDocumentation:\n\nhttps://annarchy.github.io\n\nForum:\n\nhttps://groups.google.com/forum/#!forum/annarchy\n\n\n\n\nInstallation\nInstallation guide: https://annarchy.github.io/Installation/\nUsing pip:\npip install ANNarchy\nFrom source:\ngit clone https://github.com/ANNarchy/ANNarchy.git\ncd annarchy\npip install -e .\nRequirements (Linux and MacOS):\n\ng++/clang++\npython >= 3.6\nnumpy\nsympy\ncython\n\n\n\nFeatures\n\n\n\nSimulation of both rate-coded and spiking neural networks.\nOnly local biologically realistic mechanisms are possible (no backpropagation).\nEquation-oriented description of neural/synaptic dynamics (à la Brian).\nCode generation in C++, parallelized using OpenMP on CPU and CUDA on GPU (MPI is coming).\nSynaptic, intrinsic and structural plasticity mechanisms.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStructure of a script\nfrom ANNarchy import *\n\n# Create neuron types\nneuron = Neuron(...) \n\n# Create synapse types for transmission and/or plasticity\nstdp = Synapse(...) \n\n# Create populations of neurons\npop = Population(1000, neuron) \n\n# Connect the populations through projections\nproj = Projection(pop, pop, 'exc', stdp) \nproj.connect_fixed_probability(weights=Uniform(0.0, 1.0), probability=0.1)\n\n# Generate and compile the code\ncompile() \n\n# Record spiking activity\nm = Monitor(pop, ['spike']) \n\n# Simulate for 1 second\nsimulate(1000.)\n\n\n3 - Rate-coded networks\n\n\nEcho-State Network\n\n\n\n\n\n\n\n\n\nESN rate-coded neurons follow first-order ODEs:\n\n\n    \\tau \\frac{dx(t)}{dt} + x(t) = \\sum w^\\text{in} \\, r^\\text{in}(t) + g \\, \\sum w^\\text{rec} \\, r(t) + \\xi(t)\n\n\n    r(t) = \\tanh(x(t))\n\n\n\n\nNeural dynamics are described by the equation-oriented interface:\n\nfrom ANNarchy import *\n\nESN_Neuron = Neuron(\n    parameters = \"\"\"\n        tau = 30.0   : population   # Time constant\n        g = 1.0      : population   # Scaling\n        noise = 0.01 : population   # Noise level\n    \"\"\",\n    equations=\"\"\"\n        tau * dx/dt + x = sum(in) + g * sum(exc) + noise * Uniform(-1, 1)  : init=0.0\n \n        r = tanh(x)\n    \"\"\"\n)\n\n\nParameters\n    parameters = \"\"\"\n        tau = 30.0   : population   # Time constant\n        g = 1.0      : population   # Scaling\n        noise = 0.01 : population   # Noise level\n    \"\"\"\n\nAll parameters used in the equations must be declared in the Neuron definition.\nParameters can have one value per neuron in the population (default) or be common to all neurons (flag population or projection).\nParameters and variables are double floats by default, but the type can be specified (int, bool).\n\n\n\nVariables\n    equations=\"\"\"\n        tau * dx/dt + x = sum(in) + g * sum(exc) + noise * Uniform(-1, 1) : init=0.0\n\n        r = tanh(x)\n    \"\"\"\n\nVariables are evaluated at each time step in the order of their declaration, except for coupled ODEs.\nThe output variable of a rate-coded neuron must be named r.\nVariables can be updated with assignments (=, +=, etc) or by defining first order ODEs.\nThe math C library symbols can be used (tanh, cos, exp, etc).\nInitial values at the start of the simulation can be specified with init (default: 0.0).\nLower/higher bounds on the values of the variables can be set with the min/max flags:\n\nr = x : min=0.0 # ReLU\n\nAdditive noise can be drawn from several distributions, including Uniform, Normal, LogNormal, Exponential, Gamma…\n\n\n\nODEs\n\n\n\nFirst-order ODEs are parsed and manipulated using sympy:\n\n    # All equivalent:\n    tau * dx/dt + x = I\n    tau * dx/dt = I - x\n    dx/dt = (I - x)/tau\n\nThe generated C++ code applies a numerical method (fixed step size dt) for all neurons:\n\n#pragma omp simd\nfor(unsigned int i = 0; i < size; i++){\n    double _x = (I[i] - x[i])/tau;\n    x[i] += dt*_x ;\n    r[i] = tanh(x[i]);\n}\n\n\nSeveral numerical methods are available:\n\nExplicit (forward) Euler (default):\n\ntau * dx/dt + x = I : explicit\n\nImplicit (backward) Euler:\n\ntau * dx/dt + x = I : implicit\n\nExponential Euler (exact for linear ODE):\n\ntau * dx/dt + x = I : exponential\n\nMidpoint (RK2):\n\ntau * dx/dt + x = I : midpoint\n\nEvent-driven (spiking synapses):\n\ntau * dx/dt + x = I : event-driven\n\n\n\n\nhttps://annarchy.github.io/manual/NumericalMethods/\n\n\n\nPopulations\n\nPopulations are creating by specifying a number of neurons and a neuron type:\n\npop = Population(1000, ESN_Neuron)\n\nFor visualization purposes or when using convolutional layers, a tuple geometry can be passed instead of the size:\n\npop = Population((100, 100), ESN_Neuron)\n\nAll parameters and variables become attributes of the population (read and write) as numpy arrays:\n\npop.tau = np.linspace(20.0, 40.0, 1000)\npop.r = np.tanh(pop.v)\n\nSlices of populations are called PopulationView and can be addressed separately:\n\npop = Population(1000, ESN_Neuron)\nE = pop[:800]\nI = pop[800:]\n\n\nProjections\n\nProjections connect two populations (or views) in a uni-directional way.\n\nproj_exc = Projection(E, pop, 'exc')\nproj_inh = Projection(I, pop, 'inh')\n\nEach target ('exc', 'inh', 'AMPA', 'NMDA', 'GABA') can be defined as needed and will be treated differently by the post-synaptic neurons.\nThe weighted sum of inputs for a specific target is accessed in the equations by sum(target):\n\n    equations=\"\"\"\n        tau * dx/dt + x = sum(exc) - sum(inh)\n\n        r = tanh(x)\n    \"\"\"\n\nIt is therefore possible to model modulatory effects, divisive inhibition, etc.\n\n\n\nConnection methods\n\nProjections must be populated with a connectivity matrix (who is connected to who), a weight w and optionally a delay d (uniform or variable).\nSeveral patterns are predefined:\n\nproj.connect_all_to_all(weights=Normal(0.0, 1.0), delays=2.0, allow_self_connections=False)\n\nproj.connect_one_to_one(weights=1.0, delays=Uniform(1.0, 10.0))\n\nproj.connect_fixed_number_pre(number=20, weights=1.0)\n\nproj.connect_fixed_number_post(number=20, weights=1.0)\n\nproj.connect_fixed_probability(probability=0.2, weights=1.0)\n\nproj.connect_gaussian(amp=1.0, sigma=0.2, limit=0.001)\n\nproj.connect_dog(amp_pos=1.0, sigma_pos=0.2, amp_neg=0.3, sigma_neg=0.7, limit=0.001)\n\nBut you can also load Numpy arrays or Scipy sparse matrices. Example for synfire chains:\n\n\n\nw = np.array([[None]*pre.size]*post.size)\n\nfor i in range(post.size):\n    w[i, (i-1)%pre.size] = 1.0\n\nproj.connect_from_matrix(w)\n\nw = lil_matrix((pre.size, post.size))\n\nfor i in range(pre.size):\n    w[pre.size, (i+1)%post.size] = 1.0\n\nproj.connect_from_sparse(w)\n\n\n\n\nCompiling and running the simulation\n\nOnce all populations and projections are created, you have to generate to the C++ code and compile it:\n\ncompile()\n\nYou can now manipulate all parameters/variables from Python thanks to the Cython bindings.\nA simulation is simply run for a fixed duration in milliseconds with:\n\nsimulate(1000.) # 1 second\n\nYou can also run a simulation until a criteria is filled, check:\n\nhttps://annarchy.github.io/manual/Simulation/#early-stopping\n\n\nMonitoring\n\nBy default, a simulation is run in C++ without interaction with Python.\nYou may want to record some variables (neural or synaptic) during the simulation with a Monitor:\n\nm = Monitor(pop, ['v', 'r'])\n\nn = Monitor(proj, ['w'])\n\nAfter the simulation, you can retrieve the recordings with:\n\nrecorded_v = m.get('v')\n\nrecorded_r = m.get('r')\n\nrecorded_w = n.get('w')\n\n\n\n\n\n\nWarning\n\n\n\nCalling get() flushes the underlying arrays.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nRecording projections can quickly fill up the RAM…\n\n\n\n\nNotebook: Echo-State Network\nDownload the Jupyter notebook: RC.ipynb\nRun it directly on colab: RC.ipynb\n\n\n\n\n\n\n\n\n\nPlasticity in rate-coded networks\n\nSynapses can also implement plasticity rules that will be evaluated after each neural update.\nExample of the Intrator & Cooper BCM learning rule:\n\n\\Delta w = \\eta \\, r^\\text{pre} \\, r^\\text{post}  \\,  (r^\\text{post} - \\mathbb{E}[(r^\\text{post})^2])\nIBCM = Synapse(\n    parameters = \"\"\"\n        eta = 0.01 : projection\n        tau = 2000.0 : projection\n    \"\"\",\n    equations = \"\"\"\n        tau * dtheta/dt + theta = post.r^2 : postsynaptic, exponential\n\n        dw/dt = eta * post.r * (post.r - theta) * pre.r : min=0.0, explicit\n    \"\"\",\n    psp = \"w * pre.r\"\n)\n\nEach synapse can access pre- and post-synaptic variables with pre. and post..\nThe postsynaptic flag allows to do computations only once per post-synaptic neurons.\npsp optionally defines what will be summed by the post-synaptic neuron (e.g. psp = \"w * log(pre.r)\").\n\n\n\nPlastic projections\n\nThe synapse type just has to be passed to the Projection:\n\nproj = Projection(inp, pop, 'exc', synapse=IBCM)\n\nSynaptic variables can be accessed as lists of lists for the whole projection:\n\nproj.w\nproj.theta\nor for a single post-synaptic neuron:\nproj[10].w\n\n\nNotebook: IBCM learning rule\nDownload the Jupyter notebook: BCM.ipynb\nRun it directly on colab: BCM.ipynb\n\n\n\n\nSource: http://www.scholarpedia.org/article/BCM_theory\n\n\n\n\n\nNotebook: Reward-modulated RC network of Miconi (2017)\n\nMiconi T. (2017). Biologically plausible learning in recurrent neural networks reproduces neural dynamics observed during cognitive tasks. eLife 6:e20899. doi:10.7554/eLife.20899\n\nDownload the Jupyter notebook: Miconi.ipynb\nRun it directly on colab: Miconi.ipynb\n\n\n\n\n\n\n\n\n\n4 - Spiking networks\n\n\nSpiking neurons\n\n\n\nSpiking neurons must also define two additional fields:\n\nspike: condition for emitting a spike.\nreset: what happens after a spike is emitted (at the start of the refractory period).\n\nA refractory period in ms can also be specified.\n\n\n\n\n\n\n\n\nExample of the Leaky Integrate-and-Fire:\n\n\n    C \\, \\frac{d v(t)}{dt} = - g_L \\, (v(t) - V_L) + I(t)\n\n\n    \\text{if} \\; v(t) > V_T \\; \\text{emit a spike and reset.}\n\nLIF = Neuron(\n    parameters = \"\"\"\n        C = 200.\n        g_L = 10.\n        E_L = -70.\n        v_T = 0.\n        v_r = -58.\n        I = 0.25\n    \"\"\",\n    equations = \"\"\"\n        C * dv/dt = g_L * (E_L - v) + I : init=E_L     \n    \"\"\",\n    spike = \"v >= v_T\",\n    reset = \"v = v_r\",\n    refractory = 2.0\n)\n\n\n\n\nNotebook: AdEx neuron - Adaptive exponential Integrate-and-fire\nDownload the Jupyter notebook: AdEx.ipynb\nRun it directly on colab: AdEx.ipynb\n\n\n\n\n\n\\tau \\cdot \\frac{dv (t)}{dt} = E_l - v(t) + g_\\text{exc} (t) \\, (E_\\text{exc} - v(t)) + g_\\text{inh} (t) \\, (E_\\text{inh} - v(t)) + I(t)\n\n\nConductances / currents\n\n\n\nA pre-synaptic spike arriving to a spiking neuron increases the conductance/current g_target (e.g. g_exc or g_inh, depending on the projection).\n\nLIF = Neuron(\n    parameters = \"...\",\n    equations = \"\"\"\n        C*dv/dt = g_L*(E_L - v) + g_exc : init=E_L    \n    \"\"\",\n    spike = \"v >= v_T\",\n    reset = \"v = v_r\",\n    refractory = 2.0\n)\n\nEach spike increments instantaneously g_target from the synaptic efficiency w of the corresponding synapse.\n\ng_target += w\n\n\n\n\n\n\n\n\n\n\nConductances / currents\n\n\n\nFor exponentially-decreasing or alpha-shaped synapses, ODEs have to be introduced for the conductance/current.\nThe exponential numerical method should be preferred, as integration is exact.\n\nLIF = Neuron(\n    parameters = \"...\",\n    equations = \"\"\"\n        C*dv/dt = g_L*(E_L - v) + g_exc : init=E_L   \n\n        tau_exc * dg_exc/dt = - g_exc : exponential\n    \"\"\",\n    spike = \"v >= v_T\",\n    reset = \"v = v_r\",\n    refractory = 2.0\n)\n\n\n\n\n\n\n\n\n\n\nNotebook: Synaptic transmission\nDownload the Jupyter notebook: SynapticTransmission.ipynb\nRun it directly on colab: SynapticTransmission.ipynb\n\n\n\n\n\n\n\nNotebook: COBA - Conductance-based E/I network\nDownload the Jupyter notebook: COBA.ipynb\nRun it directly on colab: COBA.ipynb\n\n\n\n\n\n\n\n\\tau \\cdot \\frac{dv (t)}{dt} = E_l - v(t) + g_\\text{exc} (t) \\, (E_\\text{exc} - v(t)) + g_\\text{inh} (t) \\, (E_\\text{inh} - v(t)) + I(t)\n\n\nSpiking synapses : Short-term plasticity (STP)\n\nSpiking synapses can define a pre_spike field, defining what happens when a pre-synaptic spike arrives at the synapse.\ng_target is an alias for the corresponding post-synaptic conductance: it will be replaced by g_exc or g_inh depending on how the synapse is used.\nBy default, a pre-synaptic spike increments the post-synaptic conductance from w: g_target += w\n\nSTP = Synapse(\n    parameters = \"\"\"\n        tau_rec = 100.0 : projection\n        tau_facil = 0.01 : projection\n        U = 0.5\n    \"\"\",\n    equations = \"\"\"\n        dx/dt = (1 - x)/tau_rec : init = 1.0, event-driven\n        du/dt = (U - u)/tau_facil : init = 0.5, event-driven\n    \"\"\",\n    pre_spike=\"\"\"\n        g_target += w * u * x\n        x *= (1 - u)\n        u += U * (1 - u)\n    \"\"\"\n)\n\n\nNotebook: STP\nDownload the Jupyter notebook: STP.ipynb\nRun it directly on colab: STP.ipynb\n\n\n\n\n\n\n\nSpiking synapses : Example of Spike-Timing Dependent plasticity (STDP)\n\npost_spike similarly defines what happens when a post-synaptic spike is emitted.\n\nSTDP = Synapse(\n    parameters = \"\"\"\n        tau_plus = 20.0 : projection ; tau_minus = 20.0 : projection\n        A_plus = 0.01 : projection   ; A_minus = 0.01 : projection\n        w_min = 0.0 : projection     ; w_max = 1.0 : projection\n    \"\"\",\n    equations = \"\"\"\n        tau_plus  * dx/dt = -x : event-driven # pre-synaptic trace\n        tau_minus * dy/dt = -y : event-driven # post-synaptic trace\n    \"\"\",\n    pre_spike=\"\"\"\n        g_target += w\n        x += A_plus * w_max\n        w = clip(w + y, w_min , w_max)\n    \"\"\",\n    post_spike=\"\"\"\n        y -= A_minus * w_max\n        w = clip(w + x, w_min , w_max)\n    \"\"\")\n\n\nNotebook: STDP\nDownload the Jupyter notebook: STDP.ipynb\nRun it directly on colab: STDP.ipynb\n\\tau^+ \\, \\frac{d x(t)}{dt} = -x(t) \\tau^- \\, \\frac{d y(t)}{dt} = -y(t)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd much more…\n\nStandard populations (SpikeSourceArray, TimedArray, PoissonPopulation, HomogeneousCorrelatedSpikeTrains), OpenCV bindings.\nStandard neurons:\n\nLeakyIntegrator, Izhikevich, IF_curr_exp, IF_cond_exp, IF_curr_alpha, IF_cond_alpha, HH_cond_exp, EIF_cond_exp_isfa_ista, EIF_cond_alpha_isfa_ista\n\nStandard synapses:\n\nHebb, Oja, IBCM, STP, STDP\n\nParallel simulations with parallel_run.\nConvolutional and pooling layers.\nHybrid rate-coded / spiking networks.\nStructural plasticity.\nBOLD monitors.\nTensorboard visualization.\n\nRTFM: https://annarchy.github.io\n\n\n\n\n\n\n\n\nReferences\n\nBi, G., and Poo, M. (2001). Synaptic Modification by Correlated Activity: Hebb’s Postulate Revisited. Annual Review of Neuroscience 24, 139–166. doi:10.1146/annurev.neuro.24.1.139.\n\n\nBienenstock, E. L., Cooper, L. N., and Munro, P. W. (1982). Theory for the development of neuron selectivity: Orientation specificity and binocular interaction in visual cortex. Journal of Neuroscience 2, 32–48.\n\n\nBrette, R., and Gerstner, W. (2005). Adaptive Exponential Integrate-and-Fire Model as an Effective Description of Neuronal Activity. Journal of Neurophysiology 94, 3637–3642. doi:10.1152/jn.00686.2005.\n\n\nDayan, P., and Abbott, L. F. (2001). Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems. The MIT Press.\n\n\nGönner, L., Vitay, J., and Hamker, F. H. (2017). Predictive Place-Cell Sequences for Goal-Finding Emerge from Goal Memory and the Cognitive Map: A Computational Model. Frontiers in Computational Neuroscience 11, 84–84. doi:10.3389/fncom.2017.00084.\n\n\nIntrator, N., and Cooper, L. N. (1992). Objective function formulation of the BCM theory of visual cortical plasticity: Statistical connections, stability conditions. Neural Networks 5, 3–17. doi:10.1016/S0893-6080(05)80003-6.\n\n\nIzhikevich, E. M. (2003). Simple model of spiking neurons. IEEE transactions on neural networks 14, 1569–72. doi:10.1109/TNN.2003.820440.\n\n\nKriegeskorte, N., and Douglas, P. K. (2018). Cognitive computational neuroscience. Nature Neuroscience 21, 1148–1160. doi:10.1038/s41593-018-0210-5.\n\n\nOja, E. (1982). A simplified neuron model as a principal component analyzer. Journal of Mathematical Biology 15, 267–73.\n\n\nVillagrasa, F., Baladron, J., Vitay, J., Schroll, H., Antzoulatos, E. G., Miller, E. K., et al. (2018). On the Role of Cortex-Basal Ganglia Interactions for Category Learning: A Neurocomputational Approach. Journal of Neuroscience 38, 9551–9562. doi:10.1523/JNEUROSCI.0874-18.2018.\n\n\nVitay, J., Dinkelbach, H. Ü., and Hamker, F. H. (2015). ANNarchy: A code generation approach to neural simulations on parallel hardware. Frontiers in Neuroinformatics 9. doi:10.3389/fninf.2015.00019.\n\n\nVitay, J., and Hamker, F. H. (2010). A computational model of Basal Ganglia and its role in memory retrieval in rewarded visual memory tasks. Frontiers in computational neuroscience 4. doi:10.3389/fncom.2010.00013.\n\n\nVitay, J., and Hamker, F. H. (2014). Timing and expectation of reward: A neuro-computational model of the afferents to the ventral tegmental area. Frontiers in Neurorobotics 8. doi:10.3389/fnbot.2014.00004."
  },
  {
    "objectID": "RateCoded.html",
    "href": "RateCoded.html",
    "title": "Rate-coded networks",
    "section": "",
    "text": "In thus section, we will take as an example an Echo-State Network (ESN), the rate-coded variant of reservoir computing introduced by Jaeger (2001). ESNs are recurrent neural networks with fixed recurrent weights. A linear readout is usually able to apprximate any traget signal based on the dynamic recurrent activity.\n\n\n\n\n\nESN rate-coded neurons follow first-order ODEs:\n\n    \\tau \\frac{dx(t)}{dt} + x(t) = \\sum w^\\text{in} \\, r^\\text{in}(t) + g \\, \\sum w^\\text{rec} \\, r(t) + \\xi(t)\n\n\n    r(t) = \\tanh(x(t))\n\nIn ANNarchy, neural dynamics are described by the equation-oriented interface:\nfrom ANNarchy import *\n\nESN_Neuron = Neuron(\n    parameters = \"\"\"\n        tau = 30.0   : population   # Time constant\n        g = 1.0      : population   # Scaling\n        noise = 0.01 : population   # Noise level\n    \"\"\",\n    equations=\"\"\"\n        tau * dx/dt + x = sum(in) + g * sum(exc) + noise * Uniform(-1, 1)  : init=0.0\n \n        r = tanh(x)\n    \"\"\"\n)\n\n\nAll parameters used in the equations must be declared in the Neuron definition.\n    parameters = \"\"\"\n        tau = 30.0   : population   # Time constant\n        g = 1.0      : population   # Scaling\n        noise = 0.01 : population   # Noise level\n    \"\"\"\nParameters can have one value per neuron in the population (default) or be common to all neurons (flag population or projection).\nParameters and variables are double floats by default, but the type can be specified (int, bool).\n\n\n\nVariables are evaluated at each time step in the order of their declaration, except for coupled ODEs.\n    equations=\"\"\"\n        tau * dx/dt + x = sum(in) + g * sum(exc) + noise * Uniform(-1, 1) : init=0.0\n\n        r = tanh(x)\n    \"\"\"\nThe output variable of a rate-coded neuron must be named r.\nVariables can be updated with assignments (=, +=, etc) or by defining first order ODEs. The math C library symbols can be used (tanh, cos, exp, etc).\nInitial values at the start of the simulation can be specified with init (default: 0.0).\nLower/higher bounds on the values of the variables can be set with the min/max flags:\nr = x : min=0.0 # ReLU\nAdditive noise can be drawn from several distributions, including Uniform, Normal, LogNormal, Exponential, Gamma…\n\n\n\nFirst-order ODEs are parsed and manipulated using sympy:\n    # All equivalent:\n    tau * dx/dt + x = I\n    tau * dx/dt = I - x\n    dx/dt = (I - x)/tau\nThe generated C++ code applies a numerical method (fixed step size dt) for all neurons:\n#pragma omp simd\nfor(unsigned int i = 0; i < size; i++){\n    double _x = (I[i] - x[i])/tau;\n    x[i] += dt*_x ;\n    r[i] = tanh(x[i]);\n}\nSeveral numerical methods are available:\n\nExplicit (forward) Euler (default):\n\ntau * dx/dt + x = I : explicit\n\nImplicit (backward) Euler:\n\ntau * dx/dt + x = I : implicit\n\nExponential Euler (exact for linear ODE):\n\ntau * dx/dt + x = I : exponential\n\nMidpoint (RK2):\n\ntau * dx/dt + x = I : midpoint\n\nEvent-driven (spiking synapses):\n\ntau * dx/dt + x = I : event-driven\nSee https://annarchy.github.io/manual/NumericalMethods/ for more explanations.\n\n\n\nPopulations are creating by specifying a number of neurons and a neuron type:\npop = Population(1000, ESN_Neuron)\nFor visualization purposes or when using convolutional layers, a tuple geometry can be passed instead of the size:\npop = Population((100, 100), ESN_Neuron)\nAll parameters and variables become attributes of the population (read and write) as numpy arrays:\npop.tau = np.linspace(20.0, 40.0, 1000)\npop.r = np.tanh(pop.v)\nSlices of populations are called PopulationView and can be addressed separately:\npop = Population(1000, ESN_Neuron)\nE = pop[:800]\nI = pop[800:]\n\n\n\nProjections connect two populations (or views) in a uni-directional way.\nproj_exc = Projection(E, pop, 'exc')\nproj_inh = Projection(I, pop, 'inh')\nEach target ('exc', 'inh', 'AMPA', 'NMDA', 'GABA') can be defined as needed and will be treated differently by the post-synaptic neurons.\nThe weighted sum of inputs for a specific target is accessed in the equations by sum(target):\n    equations=\"\"\"\n        tau * dx/dt + x = sum(exc) - sum(inh)\n\n        r = tanh(x)\n    \"\"\"\nIt is therefore possible to model modulatory effects, divisive inhibition, etc.\n\n\n\nProjections must be populated with a connectivity matrix (who is connected to who), a weight w and optionally a delay d (uniform or variable).\nSeveral patterns are predefined:\nproj.connect_all_to_all(weights=Normal(0.0, 1.0), delays=2.0, allow_self_connections=False)\n\nproj.connect_one_to_one(weights=1.0, delays=Uniform(1.0, 10.0))\n\nproj.connect_fixed_number_pre(number=20, weights=1.0)\n\nproj.connect_fixed_number_post(number=20, weights=1.0)\n\nproj.connect_fixed_probability(probability=0.2, weights=1.0)\n\nproj.connect_gaussian(amp=1.0, sigma=0.2, limit=0.001)\n\nproj.connect_dog(amp_pos=1.0, sigma_pos=0.2, amp_neg=0.3, sigma_neg=0.7, limit=0.001)\nBut you can also load Numpy arrays or Scipy sparse matrices. Example for synfire chains:\nw = np.array([[None]*pre.size]*post.size)\n\nfor i in range(post.size):\n    w[i, (i-1)%pre.size] = 1.0\n\nproj.connect_from_matrix(w)\nw = lil_matrix((pre.size, post.size))\n\nfor i in range(pre.size):\n    w[pre.size, (i+1)%post.size] = 1.0\n\nproj.connect_from_sparse(w)\n\n\n\nOnce all populations and projections are created, you have to generate to the C++ code and compile it:\ncompile()\nYou can now manipulate all parameters/variables from Python thanks to the Cython bindings.\nA simulation is simply run for a fixed duration in milliseconds with:\nsimulate(1000.) # 1 second\nYou can also run a simulation until a criteria is filled, check https://annarchy.github.io/manual/Simulation/#early-stopping\n\n\n\nBy default, a simulation is run in C++ without interaction with Python. You may want to record some variables (neural or synaptic) during the simulation with a Monitor:\nm = Monitor(pop, ['v', 'r'])\n\nn = Monitor(proj, ['w'])\nAfter the simulation, you can retrieve the recordings with:\nrecorded_v = m.get('v')\n\nrecorded_r = m.get('r')\n\nrecorded_w = n.get('w')\n\n\n\n\n\n\nWarning\n\n\n\n\nCalling get() flushes the underlying arrays.\nRecording projections can quickly fill up the RAM…\n\n\n\n\n\n\n\n\n\nNotebook: Echo-State Network\n\n\n\nDownload the Jupyter notebook: RC.ipynb\nRun it directly on colab: RC.ipynb"
  },
  {
    "objectID": "RateCoded.html#plasticity-in-rate-coded-networks",
    "href": "RateCoded.html#plasticity-in-rate-coded-networks",
    "title": "Rate-coded networks",
    "section": "Plasticity in rate-coded networks",
    "text": "Plasticity in rate-coded networks\nSynapses can also implement plasticity rules that will be evaluated after each neural update.\nExample of the Intrator & Cooper BCM learning rule:\n\\Delta w = \\eta \\, r^\\text{pre} \\, r^\\text{post}  \\,  (r^\\text{post} - \\mathbb{E}[(r^\\text{post})^2])\nIBCM = Synapse(\n    parameters = \"\"\"\n        eta = 0.01 : projection\n        tau = 2000.0 : projection\n    \"\"\",\n    equations = \"\"\"\n        tau * dtheta/dt + theta = post.r^2 : postsynaptic, exponential\n\n        dw/dt = eta * post.r * (post.r - theta) * pre.r : min=0.0, explicit\n    \"\"\",\n    psp = \"w * pre.r\"\n)\nEach synapse can access pre- and post-synaptic variables with pre. and post.. The postsynaptic flag allows to do computations only once per post-synaptic neurons.\npsp optionally defines what will be summed by the post-synaptic neuron (e.g. psp = \"w * log(pre.r)\").\nThe synapse type just has to be passed to the Projection:\nproj = Projection(inp, pop, 'exc', synapse=IBCM)\nSynaptic variables can be accessed as lists of lists for the whole projection:\nproj.w\nproj.theta\nor for a single post-synaptic neuron:\nproj[10].w\n\n\n\n\n\n\nNotebook: IBCM learning rule (Intrator and Cooper, 1992)\n\n\n\nDownload the Jupyter notebook: BCM.ipynb\nRun it directly on colab: BCM.ipynb\n\n\n\n\n\n\n\n\nNotebook: Reward-modulated RC network of Miconi (2017)\n\n\n\nDownload the Jupyter notebook: Miconi.ipynb\nRun it directly on colab: Miconi.ipynb"
  },
  {
    "objectID": "notebooks/RC.html",
    "href": "notebooks/RC.html",
    "title": "ANNarchy tutorial",
    "section": "",
    "text": "Echo state networks\nDownload the Jupyter notebook: RC.ipynb\nRun it directly on colab: RC.ipynb\nIf you run this notebook in colab, first run this cell to install ANNarchy:\n\n!pip install ANNarchy\n\nLet’s start by importing ANNarchy.\nThe clear() command is necessary in notebooks when recreating a network. If you re-run the cells creating a network without calling clear() first, populations will add up, and the results may not be what you expect.\nsetup() sets various parameters, such as the step size dt in milliseconds. By default, dt is 1.0, so the call is not necessary here.\n\nfrom ANNarchy import *\nclear()\nsetup(dt=1.0)\n\nANNarchy 4.7 (4.7.1.5) on darwin (posix).\n\n\nEach neuron in the reservoir follows the following equations:\n\n    \\tau \\frac{dx(t)}{dt} + x(t) = \\sum_\\text{input} W^\\text{IN} \\, r^\\text{IN}(t) + g \\,  \\sum_\\text{rec} W^\\text{REC} \\, r(t) + \\xi(t)\n\n\n    r(t) = \\tanh(x(t))\n\nwhere \\xi(t) is some uniform noise.\n\nESN_Neuron = Neuron(\n    parameters = \"\"\"\n        tau = 30.0 : population\n        g = 1.0 : population\n        noise = 0.01\n    \"\"\",\n    equations=\"\"\"\n        tau * dx/dt + x = sum(in) + g * sum(exc) + noise * Uniform(-1, 1)\n\n        r = tanh(x)\n    \"\"\"\n)\n\nWe take one input neuron and a RC of 400 units.\n\n# Input population\ninp = Population(1, Neuron(parameters=\"r=0.0\"))\n\n# Recurrent population\nN = 400\npop = Population(N, ESN_Neuron)\n\n\npop.tau = 30.0\npop.g = 1.4\npop.noise = 0.01\n\nInput weights are classically uniformly distributed between -1 and 1.\nRecurrent weights are sampled from the normal distribution with mean 0 and variance g^2 / N. Here, we put the synaptic scaling g inside the neuron.\n\n# Input weights\nWi = Projection(inp, pop, 'in')\nWi.connect_all_to_all(weights=Uniform(-1.0, 1.0))\n\n# Recurrent weights\nWrec = Projection(pop, pop, 'exc')\nWrec.connect_all_to_all(weights=Normal(0., 1/np.sqrt(N)))\n\n<ANNarchy.core.Projection.Projection at 0x1482198e0>\n\n\n\ncompile()\n\n\nm = Monitor(pop, 'r')\n\nA single trial lasts 3s, with a step input between 100 and 200 ms.\n\ndef trial():\n    \"Runs two trials for a given spectral radius.\"\n\n    # Reset firing rates\n    inp.r = 0.0\n    pop.x = 0.0\n    pop.r = 0.0\n    \n    # Run the trial\n    simulate(100.)\n    inp[0].r = 1.0\n    simulate(100.0) # initial stimulation\n    inp[0].r = 0.0\n    simulate(2800.)\n    \n    data = m.get('r')\n    \n    return data\n\nWe run two trials successively to look at the chaoticity depending on g.\n\npop.g = 1.4\ndata1 = trial()\ndata2 = trial()\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nsns.set_context(\"talk\")\n\nplt.figure(figsize=(20, 8))\nplt.subplot(131)\nplt.title(\"First trial\")\nfor i in range(5):\n    plt.plot(data1[:, i], lw=2)\nplt.subplot(132)\nplt.title(\"Second trial\")\nfor i in range(5):\n    plt.plot(data2[:, i], lw=2)\nplt.subplot(133)\nplt.title(\"Difference\")\nfor i in range(5):\n    plt.plot(data1[:, i] - data2[:, i], lw=2)\n\nsns.despine()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWe can now train the readout neurons to reproduce a step signal after 2 seconds.\nFor simplicity, we just train a L1-regularized linear regression (LASSO) on the reservoir activity.\n\ntarget = np.zeros(3000)\ntarget[2000:2500] = 1.0\n\n\nfrom sklearn import linear_model\nreg = linear_model.Lasso(alpha=0.001, max_iter=10000)\nreg.fit(data1, target)\npred = reg.predict(data2)\n\n\nplt.figure(figsize=(20, 10))\nplt.plot(pred, lw=3)\nplt.plot(target, lw=3)\n\nsns.despine()\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/COBA.html",
    "href": "notebooks/COBA.html",
    "title": "ANNarchy tutorial",
    "section": "",
    "text": "Download the Jupyter notebook: COBA.ipynb\nRun it directly on colab: COBA.ipynb\n\n!pip install ANNarchy\n\nThis script reproduces the benchmark used in:\n\nBrette, R., Rudolph, M., Carnevale, T., Hines, M., Beeman, D., Bower, J. M., et al. (2007), Simulation of networks of spiking neurons: a review of tools and strategies., J. Comput. Neurosci., 23, 3, 349–98\n\nbased on the balanced network proposed by:\n\nVogels, T. P. and Abbott, L. F. (2005), Signal propagation and logic gating in networks of integrate-and-fire neurons., J. Neurosci., 25, 46, 10786–95\n\nThe network is composed of 4000 neurons (3200 excitatory and 800 inhibitory), reciprocally connected with a probability of 0.02 (sparse connectivity).\nThe COBA model uses conductance-based IF neurons:\n\\tau \\cdot \\frac{dv (t)}{dt} = E_l - v(t) + g_\\text{exc} (t) \\, (E_\\text{exc} - v(t)) + g_\\text{inh} (t) \\, (E_\\text{inh} - v(t)) + I(t)\nThe discretization step has to be set to 0.1 ms:\n\nfrom ANNarchy import * \nsetup(dt=0.1) \n\nANNarchy 4.7 (4.7.1.5) on darwin (posix).\n\n\n\n\n\nCOBA = Neuron(\n    parameters=\"\"\"\n        El = -60.0          : population\n        Vr = -60.0          : population\n        Erev_exc = 0.0      : population\n        Erev_inh = -80.0    : population\n        Vt = -50.0          : population\n        tau = 20.0          : population\n        tau_exc = 5.0       : population\n        tau_inh = 10.0      : population\n        I = 20.0            : population\n    \"\"\",\n    equations=\"\"\"\n        tau * dv/dt = (El - v) + g_exc * (Erev_exc - v) + g_inh * (Erev_inh - v ) + I\n\n        tau_exc * dg_exc/dt = - g_exc\n        tau_inh * dg_inh/dt = - g_inh\n    \"\"\",\n    spike = \"v > Vt\",\n    reset = \"v = Vr\",\n    refractory = 5.0\n)\n\nThe neuron defines exponentially-decreasing conductance g_exc and g_inh for the excitatory and inhibitory conductances, respectively.\nIt also defines a refractory period of 5 ms.\n\n\n\n\nP = Population(geometry=4000, neuron=COBA)\nPe = P[:3200]\nPi = P[3200:]\n\nWe create a population of 4000 COBA neurons, and assign the 3200 first ones to the excitatory population and the 800 last ones to the inhibitory population.\nIt would have been equivalent to declare two separate populations as:\nPe = Population(geometry=3200, neuron=COBA)\nPi = Population(geometry= 800, neuron=COBA)\nbut splitting a global population allows to apply methods to all neurons, for example when recording all spikes with a single monitor, or when initializing populations parameters uniformly:\n\nP.v = Normal(-55.0, 5.0)\nP.g_exc = Normal(4.0, 1.5)\nP.g_inh = Normal(20.0, 12.0)\n\n\n\n\nThe neurons are randomly connected with a probability of 0.02. Excitatory neurons project on all other neurons with the target “exc” and a weight of 0.6, while the inhibitory neurons have the target “inh” and a weight of 6.7.\n\nCe = Projection(pre=Pe, post=P, target='exc')\nCe.connect_fixed_probability(weights=0.6, probability=0.02)\n\nCi = Projection(pre=Pi, post=P, target='inh')\nCi.connect_fixed_probability(weights=6.7, probability=0.02)\n\n<ANNarchy.core.Projection.Projection at 0x117af1e20>\n\n\n\ncompile()\n\n\n\n\nWe first define a monitor to record the spikes emitted in the whole population:\n\nm = Monitor(P, ['spike'])\n\nWe can then simulate for 100 millisecond:\n\nsimulate(100.)\n\nWe retrieve the recorded spikes from the monitor:\n\ndata = m.get('spike')\n\nand compute a raster plot from the data:\n\nt, n = m.raster_plot(data)\n\nt and n are lists representing for each spike emitted during the simulation the time at which it was emitted and the index the neuron which fired. The length of this list represents the total number of spikes in the population, so we can compute the population mean firing rate:\n\nprint('Mean firing rate in the population: ' + str(len(t) / 4000.) + 'Hz')\n\nMean firing rate in the population: 1.9125Hz\n\n\nFinally, we can show the raster plot with pylab:\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nsns.set_context(\"talk\")\n\nplt.figure(figsize=(10, 8))\nplt.plot(t, n, '.')\nplt.xlabel('Time (ms)')\nplt.ylabel('# neuron')\n\n\nsns.despine()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWe can also plot the mean firing rate in the population over time:\n\nrate = m.population_rate(data)\n\nplt.figure(figsize=(10, 6))\nplt.plot(np.linspace(0, 100, 1001), rate)\nplt.xlabel('Time (ms)')\nplt.ylabel('Population firing rate')\n\nsns.despine()\nplt.tight_layout()"
  },
  {
    "objectID": "notebooks/Miconi.html",
    "href": "notebooks/Miconi.html",
    "title": "ANNarchy tutorial",
    "section": "",
    "text": "Miconi network\nDownload the Jupyter notebook: Miconi.ipynb\nRun it directly on colab: Miconi.ipynb\nReward-modulated recurrent network based on:\n\nMiconi T. (2017). Biologically plausible learning in recurrent neural networks reproduces neural dynamics observed during cognitive tasks. eLife 6:e20899. doi:10.7554/eLife.20899\n\n\n!pip install ANNarchy\n\n\nfrom ANNarchy import *\nclear()\nsetup(dt=1.0)\n\nEach neuron in the reservoir follows the following equations:\n\n    \\tau \\frac{dx(t)}{dt} + x(t) = \\sum_\\text{input} W^\\text{IN} \\, r^\\text{IN}(t) + \\sum_\\text{rec} W^\\text{REC} \\, r(t) + \\xi(t)\n\n\n    r(t) = \\tanh(x(t))\n\nwhere \\xi(t) is a random perturbation at 3 Hz, with an amplitude randomly sampled between -A and +A.\nWe additionally keep track of the mean firing rate with a sliding average:\n\n    \\tilde{x}(t) = \\alpha \\, \\tilde{x}(t) + (1 - \\alpha) \\, x(t)\n\nThe three first neurons keep a constant rate throughout learning (1 or -1) to provide some bias to the other neurons.\n\nneuron = Neuron(\n    parameters = \"\"\"\n        tau = 30.0 : population # Time constant\n        constant = 0.0 # The four first neurons have constant rates\n        alpha = 0.05 : population # To compute the sliding mean\n        f = 3.0 : population # Frequency of the perturbation\n        A = 16. : population # Perturbation amplitude. dt*A/tau should be 0.5...\n    \"\"\",\n    equations=\"\"\"\n        # Perturbation\n        perturbation = if Uniform(0.0, 1.0) < f/1000.: 1.0 else: 0.0 \n        noise = if perturbation > 0.5: A*Uniform(-1.0, 1.0) else: 0.0\n\n        # ODE for x\n        x += dt*(sum(in) + sum(exc) - x + noise)/tau\n\n        # Output r\n        rprev = r\n        r = if constant == 0.0: tanh(x) else: tanh(constant)\n\n        # Sliding mean\n        delta_x = x - x_mean\n        x_mean = alpha * x_mean + (1 - alpha) * x\n    \"\"\"\n)\n\nThe learning rule is defined by a trace e_{i, j}(t) for each synapse i \\rightarrow j incremented at each time step with:\n\n    e_{i, j}(t) = e_{i, j}(t-1) + (r_i (t) \\, x_j(t))^3\n\nAt the end T of a trial, a normalized reward (R -R_\\text{mean}) is delivered and all weights are updated using:\n\n    \\Delta w_{i, j} = - \\eta \\,  e_{i, j}(T) \\, (R -R_\\text{mean})\n\nAll traces are then reset to 0 for the next trial. Weight changes are clamped between -0.0003 and 0.0003.\nAs ANNarchy applies the synaptic equations at each time step, we need to introduce a boolean learning_phase which performs trace integration when 0, weight update when 1.\n\nsynapse = Synapse(\n    parameters=\"\"\"\n        eta = 0.5 : projection # Learning rate\n        learning_phase = 0.0 : projection # Flag to allow learning only at the end of a trial\n        error = 0.0 : projection # Reward received\n        mean_error = 0.0 : projection # Mean Reward received\n        max_weight_change = 0.0003 : projection # Clip the weight changes\n    \"\"\",\n    equations=\"\"\"\n        # Trace\n        trace += if learning_phase < 0.5:\n                    power(pre.rprev * (post.delta_x), 3)\n                 else:\n                    0.0\n\n        # Weight update only at the end of the trial\n        delta_w = if learning_phase > 0.5:\n                eta * trace * (mean_error) * (error - mean_error)\n             else:\n                 0.0 : min=-max_weight_change, max=max_weight_change\n        w -= if learning_phase > 0.5:\n                delta_w\n             else:\n                 0.0\n    \"\"\"\n)\n\nWe model the DNMS task of Miconi. The RC network has two inputs A and B. The reservoir has 200 neurons, 3 of which have constant rates.\n\n# Input population\ninp = Population(2, Neuron(parameters=\"r=0.0\"))\n\n# Recurrent population\nN = 200\npop = Population(N, neuron)\npop[0].constant = 1.0\npop[1].constant = 1.0\npop[2].constant = -1.0\npop.x = Uniform(-0.1, 0.1)\n\nInput weights are uniformly distributed between -1 and 1.\nRecurrent weights and normally distributed, with a coupling strength of g=1.5 (edge of chaos).\nConnections are all-to-all (fully connected).\n\n# Input weights\nWi = Projection(inp, pop, 'in')\nWi.connect_all_to_all(weights=Uniform(-1.0, 1.0))\n\n# Recurrent weights\ng = 1.5\nWrec = Projection(pop, pop, 'exc', synapse)\nWrec.connect_all_to_all(weights=Normal(0., g/np.sqrt(N)), allow_self_connections=True)\n\n<ANNarchy.core.Projection.Projection at 0x11959cc70>\n\n\n\ncompile()\n\nThe output of the reservoir is chosen to be the neuron of index 100.\n\noutput_neuron = 100\n\nWe record the rates inside the reservoir:\n\nm = Monitor(pop, ['r'])\n\nParameters defining the task:\n\n# Compute the mean reward per trial\nR_mean = np.zeros((2, 2))\nalpha = 0.75 # 0.33\n\n# Durations\nd_stim = 200\nd_delay= 200\nd_response = 400\nd_execution= 200\n\nDefinition of a DNMS trial (AA, AB, BA, BB):\n\ndef dnms_trial(trial, first, second, printing=False):\n    global R_mean\n    traces = []\n\n    # Reinitialize network\n    pop.x = Uniform(-0.1, 0.1).get_values(N)\n    pop.r = np.tanh(pop.x)\n    pop[0].r = np.tanh(1.0)\n    pop[1].r = np.tanh(1.0)\n    pop[2].r = np.tanh(-1.0)\n\n    # First input\n    inp[first].r = 1.0\n    simulate(d_stim)\n    \n    # Delay\n    inp.r = 0.0\n    simulate(d_delay)\n    \n    # Second input\n    inp[second].r = 1.0\n    simulate(d_stim)\n    \n    # Relaxation\n    inp.r = 0.0\n    simulate(d_response)\n    \n    # Read the output\n    rec = m.get()\n    \n    # Compute the target\n    target = 0.98 if first != second else -0.98\n    \n    # Response is over the last 200 ms\n    output = rec['r'][-int(d_execution):, output_neuron] # neuron 100 over the last 200 ms\n    \n    # Compute the error\n    error = np.mean(np.abs(target - output))\n    if printing:\n        print('Target:', target, '\\tOutput:', \"%0.3f\" % np.mean(output), '\\tError:',  \"%0.3f\" % error, '\\tMean:', \"%0.3f\" % R_mean[first, second])\n    \n    # The first 25 trial do not learn, to let R_mean get realistic values\n    if trial > 25:\n\n        # Apply the learning rule\n        Wrec.learning_phase = 1.0\n        Wrec.error = error\n        Wrec.mean_error = R_mean[first, second]\n\n        # Learn for one step\n        step()\n        \n        # Reset the traces\n        Wrec.learning_phase = 0.0\n        Wrec.trace = 0.0\n        _ = m.get() # to flush the recording of the last step\n\n    # Update the mean reward\n    R_mean[first, second] = alpha * R_mean[first, second] + (1.- alpha) * error\n\n    return rec, traces\n\n\nfrom IPython.display import clear_output\nprinting = False\n\n# Many trials of each type\nmean_rewards = []\ntry:\n    for trial in range(1500):\n        if printing:\n            clear_output(wait=True)\n            print('Trial', trial)\n\n        # Perform the four different trials successively\n        recordsAA, tracesAA = dnms_trial (trial, 0, 0, printing)\n        recordsAB, tracesAB = dnms_trial (trial, 0, 1, printing)\n        recordsBA, tracesBA = dnms_trial (trial, 1, 0, printing)\n        recordsBB, tracesBB = dnms_trial (trial, 1, 1, printing)\n\n        # Record the initial trial\n        if trial == 0:\n            initialAA = recordsAA['r']\n            initialAB = recordsAB['r']\n            initialBA = recordsBA['r']\n            initialBB = recordsBB['r']\n\n        mean_rewards.append(R_mean.copy())\n\nexcept KeyboardInterrupt:\n    pass\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nsns.set_context(\"talk\")\n\nmean_rewards = np.array(mean_rewards)\n\nplt.figure(figsize=(10, 6))\nplt.plot(mean_rewards.mean(axis=(1,2)), label='mean')\nplt.plot(mean_rewards[:, 0, 0], label='AA')\nplt.plot(mean_rewards[:, 0, 1], label='AB')\nplt.plot(mean_rewards[:, 1, 0], label='BA')\nplt.plot(mean_rewards[:, 1, 1], label='BB')\nplt.xlabel(\"Trials\")\nplt.ylabel(\"Mean error\")\nplt.legend()\n\nplt.figure(figsize=(10, 8))\n\nax = plt.subplot(221)\nax.plot(np.mean(initialAA[:, output_neuron:output_neuron+1], axis=1), label='before')\nax.plot(np.mean(recordsAA['r'][:, output_neuron:output_neuron+1], axis=1), label='after')\nax.set_ylim((-1., 1.))\nax.legend()\nax.set_title('Output AA -1')\nax = plt.subplot(222)\nax.plot(np.mean(initialBA[:, output_neuron:output_neuron+1], axis=1), label='before')\nax.plot(np.mean(recordsBA['r'][:, output_neuron:output_neuron+1], axis=1), label='after')\nax.set_ylim((-1., 1.))\nax.legend()\nax.set_title('Output BA +1')\nax = plt.subplot(223)\nax.plot(np.mean(initialAB[:, output_neuron:output_neuron+1], axis=1), label='before')\nax.plot(np.mean(recordsAB['r'][:, output_neuron:output_neuron+1], axis=1), label='after')\nax.set_ylim((-1., 1.))\nax.set_title('Output AB +1')\nax = plt.subplot(224)\nax.plot(np.mean(initialBB[:, output_neuron:output_neuron+1], axis=1), label='before')\nax.plot(np.mean(recordsBB['r'][:, output_neuron:output_neuron+1], axis=1), label='after')\nax.set_ylim((-1., 1.))\nax.set_title('Output BB -1')\nplt.show()"
  },
  {
    "objectID": "notebooks/AdEx.html",
    "href": "notebooks/AdEx.html",
    "title": "ANNarchy tutorial",
    "section": "",
    "text": "Adaptive Exponential IF neuron\nDownload the Jupyter notebook: AdEx.ipynb\nRun it directly on colab: AdEx.ipynb\nThis notebook explores how the AdEx neuron model can reproduce various spiking patterns observed in vivo.\nCode based on:\n\nNaud, R., Marcille, N., Clopath, C., and Gerstner, W. (2008). Firing patterns in the adaptive exponential integrate-and-fire model. Biol Cybern 99, 335. doi:10.1007/s00422-008-0264-7.\n\nOn colab:\n\n!pip install ANNarchy\n\n\nfrom ANNarchy import *\nclear()\nsetup(dt=0.1)\n\nANNarchy 4.7 (4.7.1.5) on darwin (posix).\n\n\nThe AdEx neuron is defined by the following equations:\n\n    C \\, \\frac{dv}{dt} = -g_L \\ (v - E_L) + g_L \\, \\Delta_T \\, \\exp(\\frac{v - v_T}{\\Delta_T}) + I - w\n\n\n    \\tau_w \\, \\frac{dw}{dt} = a \\, (v - E_L) - w\n\nif v > v_\\text{spike}:\n\nv = v_R\nw = w + b\n\n\nAdEx = Neuron(\n    parameters=\"\"\"\n        C = 200.\n        gL = 10. # not g_L! g_ is reserved for spike transmission\n        E_L = -70.\n        v_T = -50.\n        delta_T = 2.0\n        a = 2.0\n        tau_w = 30.\n        b = 0.\n        v_r = -58.\n        I = 500.\n        v_spike = 0.0 \n    \"\"\",\n    equations=\"\"\"\n        C * dv/dt = - gL * (v - E_L) +  gL * delta_T * exp((v-v_T)/delta_T) + I - w : init=-70.0     \n        tau_w * dw/dt = a * (v - E_L) - w  : init=0.0\n    \"\"\",\n    spike=\"\"\"\n        v >= v_spike\n    \"\"\",\n    reset=\"\"\"\n        v = v_r\n        w += b\n    \"\"\",\n    refractory = 2.0\n)\n\nWe create a population of 8 AdEx neurons which will get different parameter values.\n\npop = Population(8, AdEx)\n\n\ncompile()\n\nWe add a monitor to track the membrane potential and the spike timings during the simulation.\n\nm = Monitor(pop, ['v', 'spike'])\n\nAs in the paper, we provide different parameters to each neuron and simulate the network for 500 ms with a fixed input current, and remove that current for an additional 50 ms.\n\n# a) tonic spiking b) adaptation, c) initial burst, d) regular bursting, e) delayed accelerating, f) delayed regular bursting, g) transcient spiking, h) irregular spiking\npop.C =       [200, 200, 130, 200, 200, 200, 100, 100]\npop.gL =      [ 10,  12,  18,  10,  12,  12,  10,  12]\npop.E_L =     [-70, -70, -58, -58, -70, -70, -65, -60]\npop.v_T =     [-50, -50, -50, -50, -50, -50, -50, -50]\npop.delta_T = [  2,   2,   2,   2,   2,   2,   2,   2]\npop.a =       [  2,   2,   4,   2,-10., -6.,-10.,-11.]\npop.tau_w =   [ 30, 300, 150, 120, 300, 300,  90, 130]\npop.b =       [  0,  60, 120, 100,   0,   0,  30,  30]\npop.v_r =     [-58, -58, -50, -46, -58, -58, -47, -48]\npop.I =       [500, 500, 400, 210, 300, 110, 350, 160]\n\n# Reset neuron\npop.v = pop.E_L\npop.w = 0.0\n\n# Simulate\nsimulate(500.)\npop.I = 0.0\nsimulate(50.)\n\n# Recordings\ndata = m.get('v')\nspikes = m.get('spike')\nfor n, t in spikes.items(): # Normalize the spikes\n    data[[x - m.times()['v']['start'][0] for x in t], n] = 0.0\n\nWe can now visualize the simulations:\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nsns.set_context(\"talk\")\n\ntitles = [\n    \"a) tonic spiking\", \n    \"b) adaptation\", \n    \"c) initial burst\", \n    \"d) regular bursting\", \n    \"e) delayed accelerating\", \n    \"f) delayed regular bursting\", \n    \"g) transcient spiking\", \n    \"h) irregular spiking\"\n]\n\nplt.figure(figsize=(20, 20))\nplt.ylim((-70., 0.))\nfor i in range(8):\n    plt.subplot(4, 2, i+1)\n    plt.title(titles[i])\n    plt.plot(data[:, i], lw=3)\n    \n\nsns.despine()\nplt.tight_layout()\nplt.show()\n\n/var/folders/6w/6msx49ws7k13cc0bbys0tt4m0000gn/T/ipykernel_72075/1457585605.py:21: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\n  plt.subplot(4, 2, i+1)"
  },
  {
    "objectID": "notebooks/SynapticTransmission.html",
    "href": "notebooks/SynapticTransmission.html",
    "title": "ANNarchy tutorial",
    "section": "",
    "text": "Synaptic transmission\nDownload the Jupyter notebook: SynapticTransmission.ipynb\nRun it directly on colab: SynapticTransmission.ipynb\nThis notebook simply demonstrates the three main type of synaptic transmission for spiking neurons:\n\nInstantaneous\nExponentially-decreasing\nAlpha-shaped\n\n\n!pip install ANnarchy\n\n\nfrom ANNarchy import *\nclear()\n\nANNarchy 4.7 (4.7.1.5) on darwin (posix).\n\n\nWe use here a simple LIF neuron receving three types of projections (a, b, c). The conductance g_a uses instantaneous transmission, as it is reset to 0 after each step. g_b decreases exponentially with time following a first order ODE. g_c is integrated twice in alpha_c, leading to the alpha shape.\nAll methods use the exponential numerical method, as they are first order linear ODEs and can be solved exactly.\n\nLIF = Neuron(\n    parameters=\"\"\"\n        tau = 20.\n        E_L = -70.\n        v_T = 0.\n        v_r = -58.\n        tau_b = 10.0\n        tau_c = 10.0\n    \"\"\",\n    equations=\"\"\"\n        # Membrane potential\n        tau * dv/dt = (E_L - v) + g_a + g_b + alpha_c : init=-70.\n        \n        # Exponentially decreasing\n        tau_b * dg_b/dt = -g_b : exponential\n        \n        # Alpha-shaped\n        tau_c * dg_c/dt = -g_c : exponential\n        tau_c * dalpha_c/dt = exp((tau_c - dt/2.0)/tau_c) * g_c - alpha_c  : exponential\n    \"\"\",\n    spike=\"v >= v_T\",\n    reset=\"v = v_r\",\n    refractory = 2.0\n)\n\nThe LIF neuron will receive a single spike at t = 10ms, using the SpikeSourceArray specific population.\n\ninp = SpikeSourceArray([10.])\npop = Population(1, LIF)\n\nWe implement three different projections between the same neurons, to highlight the three possible transmission mechanisms.\n\nproj = Projection(inp, pop, 'a').connect_all_to_all(weights=1.0)\nproj = Projection(inp, pop, 'b').connect_all_to_all(weights=1.0)\nproj = Projection(inp, pop, 'c').connect_all_to_all(weights=1.0)\n\n\ncompile()\n\nCompiling ...  OK \n\n\nWe monitor the three conductances:\n\nm = Monitor(pop, ['g_a', 'g_b', 'alpha_c'])\n\n\ninp.clear()\nsimulate(100.)\n\n\ndata = m.get()\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nsns.set_context(\"talk\")\n\nplt.figure(figsize=(10, 10))\nplt.subplot(311)\nplt.plot(data['g_a'][:, 0])\nplt.ylabel(\"Instantaneous\")\nplt.subplot(312)\nplt.plot(data['g_b'][:, 0])\nplt.ylabel(\"Exponential\")\nplt.subplot(313)\nplt.plot(data['alpha_c'][:, 0])\nplt.xlabel(\"Time (ms)\")\nplt.ylabel(\"Alpha\")\nplt.show()"
  },
  {
    "objectID": "notebooks/BCM.html",
    "href": "notebooks/BCM.html",
    "title": "ANNarchy tutorial",
    "section": "",
    "text": "BCM learning rule\nDownload the Jupyter notebook: BCM.ipynb\nRun it directly on colab: BCM.ipynb\nThe goal of this notebook is to investigate the Intrator & Cooper BCM learning rule for rate-coded networks.\n\\Delta w = \\eta \\, r^\\text{pre} \\, r^\\text{post}  \\,  (r^\\text{post} - \\mathbb{E}[(r^\\text{post})^2])\n\nIntrator, N., & Cooper, L. N. (1992). Objective function formulation of the BCM theory of visual cortical plasticity: Statistical connections, stability conditions. Neural Networks, 5(1), 3–17. https://doi.org/10.1016/S0893-6080(05)80003-6\n\n\n!pip install ANNarchy\n\nWe can now import ANNarchy:\n\nfrom ANNarchy import *\nclear()\nsetup(dt=1.0)\n\nANNarchy 4.7 (4.7.1.5) on darwin (posix).\n\n\nWe will keep a minimal experimental setup, with two input neurons connected to a single output neuron. Note how the input neurons are defined by setting r as a parameter that can be set externally.\n\n# Input\ninput_neuron = Neuron(\n    parameters = \"\"\"\n        r = 0.0\n    \"\"\"\n)\npre = Population(2, input_neuron)\n\n# Output\nneuron = Neuron(\n    equations = \"\"\"\n        r = sum(exc)\n    \"\"\"\n)\npost = Population(1, neuron)\n\nWe can now define a synapse model implementing the Intrator and Cooper version of the BCM learning rule.\nThe synapse has two parameters: The learning rate eta and the time constant tau of the moving average theta. Both are defined as projection parameters, as we only need one value for the whole projection. If you omit this flag, there will be one value per synapse, which would be a waste of RAM.\nThe moving average theta tracks the square of the post-synaptic firing rate post.r. It has the flag postsynaptic, as we need to compute only one variable per post-synaptic neuron (it does not really matter in our example as have only one output neuron…). It uses the exponential numerical method, as it is a first-order linear ODE that can be solved exactly. However, the default explicit Euler method would work just as well here.\nThe weight change dw/dt follows the BCM learning rule. min=0.0 ensures that the weight w stays positive throughout learning. The explicit Euler method is the default and could be omitted.\nThe psp argument w * pre.r (what is summed by the post-synaptic neuron over its incoming connections) is also the default value and could be omitted.\n\nIBCM = Synapse(\n    parameters = \"\"\"\n        eta = 0.01 : projection\n        tau = 100.0 : projection\n    \"\"\",\n    equations = \"\"\"\n        tau * dtheta/dt + theta = (post.r)^2 : postsynaptic, exponential\n\n        dw/dt = eta * post.r * (post.r - theta) * pre.r : min=0.0, explicit\n    \"\"\",\n    psp = \"w * pre.r\"\n)\n\nWe can now create a projection between the two populations using the synapse type. The connection method is all-to-all, initialozing the two weights to 1.\n\nproj = Projection(pre, post, 'exc', IBCM)\nproj.connect_all_to_all(1.0)\n\n<ANNarchy.core.Projection.Projection at 0x1198dea00>\n\n\nWe can now compile the network and record the post-synaptic firing rate as well as the evolution of the weights and thresholds during learning.\n\ncompile()\n\nm = Monitor(post, 'r')\nn = Monitor(proj, ['w', 'theta'])\n\nCompiling ...  OK \nWARNING: Monitor(): it is a bad idea to record synaptic variables of a projection at each time step! \n\n\nThe simulation protocol is kept simple, as it consists of setting constant firing rates for the two input neurons and simulating for one second.\n\npre.r = np.array([1.0, 0.1])\nsimulate(1000.)\n\nWe can now retrieve the recordings and plot the evolution of the various variables.\n\nr = m.get('r')\nw = n.get('w')\ntheta = n.get('theta')\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nsns.set_context(\"talk\")\n\nplt.figure(figsize=(10, 5))\nplt.subplot(211)\nplt.plot(r[:, 0], label='r')\nplt.plot(theta[:, 0], label='theta')\nplt.legend()\nplt.subplot(212)\nplt.plot(w[:, 0, 0], label=\"$w_1$\")\nplt.plot(w[:, 0, 1], label=\"$w_2$\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nNotice how the first weight increases when r is higher than theta (LTP), but decreases afterwards (LTD). Unintuitively, the input neuron with the highest activity sees its weight decreased at the end of the stimulation."
  },
  {
    "objectID": "notebooks/STDP.html",
    "href": "notebooks/STDP.html",
    "title": "ANNarchy tutorial",
    "section": "",
    "text": "STDP\nDownload the Jupyter notebook: STDP.ipynb\nRun it directly on colab: STDP.ipynb\nThis notebook demonstrates the online implementation of the spike time-dependent plasticity rule.\n\n!pip install ANNarchy\n\n\nfrom ANNarchy import *\nclear()\nsetup(dt=1.0)\n\nANNarchy 4.7 (4.7.1.5) on darwin (posix).\n\n\nThe STDP learning rule maintains exponentially-decaying traces for the pre-synaptic and post-synaptic spikes.\n\\tau^+ \\, \\frac{d x(t)}{dt} = -x (t)\n\\tau^- \\, \\frac{d y(t)}{dt} = -x (t)\nLTP and LTD occur at spike times depending on the corresponding traces.\n\nWhen a pre-synaptic spike occurs, x(t) is incremented and LTD is applied proportionally to y(t).\nWhen a post-synaptic spike occurs, y(t) is incremented and LTP is applied proportionally to x(t).\n\n\nSTDP = Synapse(\n    parameters = \"\"\"\n        tau_plus = 20.0 : projection ; tau_minus = 20.0 : projection\n        A_plus = 0.01 : projection   ; A_minus = 0.01 : projection\n        w_min = 0.0 : projection     ; w_max = 2.0 : projection\n    \"\"\",\n    equations = \"\"\"\n    \n        tau_plus * dx/dt = -x : event-driven # pre-synaptic trace\n    \n        tau_minus * dy/dt = -y : event-driven # post-synaptic trace\n    \n    \"\"\",\n    pre_spike=\"\"\"\n        \n        g_target += w\n        \n        x += A_plus * w_max\n        \n        w = clip(w - y, w_min , w_max) # LTD\n    \"\"\",\n    post_spike=\"\"\"\n        \n        y += A_minus * w_max\n        \n        w = clip(w + x, w_min , w_max) # LTP\n    \"\"\"\n)\n\nWe create two dummy populations with one neuron each, whose spike times we can control.\n\npre = SpikeSourceArray([[0.]])\npost = SpikeSourceArray([[50.]])\n\nWe connect the population using a STDP synapse.\n\nproj = Projection(pre, post, 'exc', STDP)\nproj.connect_all_to_all(1.0)\n\n<ANNarchy.core.Projection.Projection at 0x1170b24f0>\n\n\n\ncompile()\n\nCompiling ...  OK \n\n\nThe presynaptic neuron will fire at avrious times between 0 and 100 ms, while the postsynaptic neuron keeps firing at 50 ms.\n\npre_times = np.linspace(100.0, 0.0, 101)\n\n\nweight_changes = []\nfor t_pre in pre_times:\n    \n    # Reset the populations\n    pre.clear()\n    post.clear()\n    pre.spike_times = [[t_pre]]\n    post.spike_times = [[50.0]]\n    \n    # Reset the traces\n    proj.x = 0.0\n    proj.y = 0.0\n    \n    # Weight before the simulation\n    w_before = proj[0].w[0]\n    \n    # Simulate long enough\n    simulate(105.0)\n    \n    # Record weight change\n    delta_w = proj[0].w[0] - w_before\n    weight_changes.append(delta_w)\n\nWe can now plot the classical STDP figure:\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nsns.set_context(\"talk\")\n\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(1, 1, 1)\nplt.plot(50. - pre_times, weight_changes, \"*\")\nplt.xlabel(\"t_post - t_pre\")\nplt.ylabel(\"delta_w\")\n\nsns.despine()\n\nax.spines['left'].set_position('zero')\nax.spines['bottom'].set_position('zero')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/STP.html",
    "href": "notebooks/STP.html",
    "title": "ANNarchy tutorial",
    "section": "",
    "text": "Short-term Plasticity\nDownload the Jupyter notebook: STP.ipynb\nRun it directly on colab: STP.ipynb\nImplementation of the recurrent network proposed in:\n\nTsodyks, Uziel and Markram (2000). Synchrony Generation in Recurrent Networks with Frequency-Dependent Synapses, The Journal of Neuroscience, 20(50).\n\n\nfrom ANNarchy import *\nclear()\ndt=0.25\nsetup(dt=dt)\n\nANNarchy 4.7 (4.7.1.5) on darwin (posix).\n\n\nThis network uses simple leaky integrate-and-fire (LIF) neurons. The network is compsed of 400 excitatory and 100 inhibitory neurons, receiving increasingly strong input currents.\n\nLIF = Neuron(\n    parameters = \"\"\"\n    tau = 30.0 : population\n    I = 15.0\n    tau_I = 3.0 : population\n    \"\"\",\n    equations = \"\"\"\n    tau * dv/dt = -v + g_exc - g_inh + I : init=13.5\n    tau_I * dg_exc/dt = -g_exc\n    tau_I * dg_inh/dt = -g_inh\n    \"\"\",\n    spike = \"v > 15.0\",\n    reset = \"v = 13.5\",\n    refractory = 3.0\n)\n\nP = Population(geometry=500, neuron=LIF)\n\nP.I = np.sort(Uniform(14.625, 15.375).get_values(500))\nP.v = Uniform(0.0, 15.0)\n\nExc = P[:400]\nInh = P[400:]\n\nShort-term plasticity can be defined by dynamical changes of synaptic efficiency, based on pre- or post-synaptic activity.\nWe define a STP synapse, whose post-pynaptic potential (psp, define by g_target) depends not only on the weight w and the emission of pre-synaptic spike, but also on intra-synaptic variables x and u:\n\nSTP = Synapse(\n    parameters = \"\"\"\n    w=0.0\n    tau_rec = 1.0\n    tau_facil = 1.0\n    U = 0.1\n    \"\"\",\n    equations = \"\"\"\n    dx/dt = (1 - x)/tau_rec : init = 1.0, event-driven\n    du/dt = (U - u)/tau_facil : init = 0.1, event-driven   \n    \"\"\",\n    pre_spike=\"\"\"\n    g_target += w * u * x\n    x *= (1 - u)\n    u += U * (1 - u)\n    \"\"\"\n)\n\nCreating the projection between the excitatory and inhibitory is straightforward when the right parameters are chosen:\n\n# Parameters for the synapses\nAee = 1.8\nAei = 5.4\nAie = 7.2\nAii = 7.2\n\nUee = 0.5\nUei = 0.5\nUie = 0.04\nUii = 0.04\n\ntau_rec_ee = 800.0\ntau_rec_ei = 800.0\ntau_rec_ie = 100.0\ntau_rec_ii = 100.0\n\ntau_facil_ie = 1000.0\ntau_facil_ii = 1000.0\n\n# Create projections\nproj_ee = Projection(pre=Exc, post=Exc, target='exc', synapse=STP)\nproj_ee.connect_fixed_probability(probability=0.1, weights=Normal(Aee, (Aee/2.0), min=0.2*Aee, max=2.0*Aee)) \nproj_ee.U = Normal(Uee, (Uee/2.0), min=0.1, max=0.9)\nproj_ee.tau_rec = Normal(tau_rec_ee, (tau_rec_ee/2.0), min=5.0)\nproj_ee.tau_facil = dt # Cannot be 0!\n\nproj_ei = Projection(pre=Inh, post=Exc, target='inh', synapse=STP)\nproj_ei.connect_fixed_probability(probability=0.1, weights=Normal(Aei, (Aei/2.0), min=0.2*Aei, max=2.0*Aei))\nproj_ei.U = Normal(Uei, (Uei/2.0), min=0.1, max=0.9)\nproj_ei.tau_rec = Normal(tau_rec_ei, (tau_rec_ei/2.0), min=5.0)\nproj_ei.tau_facil = dt # Cannot be 0!\n\nproj_ie = Projection(pre=Exc, post=Inh, target='exc', synapse=STP)\nproj_ie.connect_fixed_probability(probability=0.1, weights=Normal(Aie, (Aie/2.0), min=0.2*Aie, max=2.0*Aie))\nproj_ie.U = Normal(Uie, (Uie/2.0), min=0.001, max=0.07)\nproj_ie.tau_rec = Normal(tau_rec_ie, (tau_rec_ie/2.0), min=5.0)\nproj_ie.tau_facil = Normal(tau_facil_ie, (tau_facil_ie/2.0), min=5.0)\n\nproj_ii = Projection(pre=Inh, post=Inh, target='inh', synapse=STP)\nproj_ii.connect_fixed_probability(probability=0.1, weights=Normal(Aii, (Aii/2.0), min=0.2*Aii, max=2.0*Aii))\nproj_ii.U = Normal(Uii, (Uii/2.0), min=0.001, max=0.07)\nproj_ii.tau_rec = Normal(tau_rec_ii, (tau_rec_ii/2.0), min=5.0)\nproj_ii.tau_facil = Normal(tau_facil_ii, (tau_facil_ii/2.0), min=5.0)\n\nWe compile and simulate for 10 seconds:\n\ncompile()\n\n\nMe = Monitor(Exc, 'spike')\nMi = Monitor(Inh, 'spike')\n\n\nduration = 10000.0\nsimulate(duration, measure_time=True)\n\nSimulating 10.0 seconds of the network took 0.10498785972595215 seconds. \n\n\nWe retrieve the recordings and plot them:\n\n# Retrieve recordings\ndata_exc = Me.get()\ndata_inh = Mi.get()\nte, ne = Me.raster_plot(data_exc['spike'])\nti, ni = Mi.raster_plot(data_inh['spike'])\n\n# Histogram of the exc population\nh = Me.histogram(data_exc['spike'], bins=1.0)\n\n# Mean firing rate of each excitatory neuron\nrates = []\nfor neur in data_exc['spike'].keys():\n    rates.append(len(data_exc['spike'][neur])/duration*1000.0)\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nsns.set_context(\"talk\")\n\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nplt.plot(te, ne, 'b.', markersize=1.0)\nplt.plot(ti, ni, 'b.', markersize=1.0)\nplt.xlim((0, duration)); plt.ylim((0,500))\nplt.xlabel('Time (ms)')\nplt.ylabel('# neuron')\n\nplt.subplot(3,1,2)\nplt.plot(h/400.)\nplt.xlabel('Time (ms)')\nplt.ylabel('Net activity')\n\nplt.subplot(3,1,3)\nplt.plot(sorted(rates))\nplt.ylabel('Spikes / sec')\nplt.xlabel('# neuron')\nplt.show()"
  }
]