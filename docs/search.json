[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ANNarchy - Artificial Neural Networks architect",
    "section": "",
    "text": "This website is a tutorial for beginners to get started: for more details and functionalities, please refer to the documentation. The slides used for the videos are available here. Jupyter notebooks for simple models showcasing the usage of ANNarchy are available here."
  },
  {
    "objectID": "notebooks/RC.html",
    "href": "notebooks/RC.html",
    "title": "ANNarchy tutorial",
    "section": "",
    "text": "If you run this notebook in colab, first run this cell to install ANNarchy:\n\n!pip install ANNarchy\n\nLet’s start by importing ANNarchy.\nThe clear() command is necessary in notebooks when recreating a network. If you re-run the cells creating a network without calling clear() first, populations will add up, and the results may not be what you expect.\nsetup() sets various parameters, such as the step size dt in milliseconds. By default, dt is 1.0, so the call is not necessary here.\n\nfrom ANNarchy import *\nclear()\nsetup(dt=1.0)\n\nANNarchy 4.7 (4.7.1.5) on darwin (posix).\n\n\nEach neuron in the reservoir follows the following equations:\n\n    \\tau \\frac{dx(t)}{dt} + x(t) = \\sum_\\text{input} W^\\text{IN} \\, r^\\text{IN}(t) + g \\,  \\sum_\\text{rec} W^\\text{REC} \\, r(t) + \\xi(t)\n\n\n    r(t) = \\tanh(x(t))\n\nwhere \\xi(t) is some uniform noise.\n\nESN_Neuron = Neuron(\n    parameters = \"\"\"\n        tau = 30.0 : population\n        g = 1.0 : population\n        noise = 0.01\n    \"\"\",\n    equations=\"\"\"\n        tau * dx/dt + x = sum(in) + g * sum(exc) + noise * Uniform(-1, 1)\n\n        r = tanh(x)\n    \"\"\"\n)\n\nWe take one input neuron and a RC of 400 units.\n\n# Input population\ninp = Population(1, Neuron(parameters=\"r=0.0\"))\n\n# Recurrent population\nN = 400\npop = Population(N, ESN_Neuron)\n\n\npop.tau = 30.0\npop.g = 1.4\npop.noise = 0.01\n\nInput weights are classically uniformly distributed between -1 and 1.\nRecurrent weights are sampled from the normal distribution with mean 0 and variance g^2 / N. Here, we put the synaptic scaling g inside the neuron.\n\n# Input weights\nWi = Projection(inp, pop, 'in')\nWi.connect_all_to_all(weights=Uniform(-1.0, 1.0))\n\n# Recurrent weights\nWrec = Projection(pop, pop, 'exc')\nWrec.connect_all_to_all(weights=Normal(0., 1/np.sqrt(N)))\n\n<ANNarchy.core.Projection.Projection at 0x1482198e0>\n\n\n\ncompile()\n\n\nm = Monitor(pop, 'r')\n\nA single trial lasts 3s, with a step input between 100 and 200 ms.\n\ndef trial():\n    \"Runs two trials for a given spectral radius.\"\n\n    # Reset firing rates\n    inp.r = 0.0\n    pop.x = 0.0\n    pop.r = 0.0\n    \n    # Run the trial\n    simulate(100.)\n    inp[0].r = 1.0\n    simulate(100.0) # initial stimulation\n    inp[0].r = 0.0\n    simulate(2800.)\n    \n    data = m.get('r')\n    \n    return data\n\nWe run two trials successively to look at the chaoticity depending on g.\n\npop.g = 1.4\ndata1 = trial()\ndata2 = trial()\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nsns.set_context(\"talk\")\n\nplt.figure(figsize=(20, 8))\nplt.subplot(131)\nplt.title(\"First trial\")\nfor i in range(5):\n    plt.plot(data1[:, i], lw=2)\nplt.subplot(132)\nplt.title(\"Second trial\")\nfor i in range(5):\n    plt.plot(data2[:, i], lw=2)\nplt.subplot(133)\nplt.title(\"Difference\")\nfor i in range(5):\n    plt.plot(data1[:, i] - data2[:, i], lw=2)\n\nsns.despine()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWe can now train the readout neurons to reproduce a step signal after 2 seconds.\nFor simplicity, we just train a L1-regularized linear regression (LASSO) on the reservoir activity.\n\ntarget = np.zeros(3000)\ntarget[2000:2500] = 1.0\n\n\nfrom sklearn import linear_model\nreg = linear_model.Lasso(alpha=0.001, max_iter=10000)\nreg.fit(data1, target)\npred = reg.predict(data2)\n\n\nplt.figure(figsize=(20, 10))\nplt.plot(pred, lw=3)\nplt.plot(target, lw=3)\n\nsns.despine()\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/COBA.html",
    "href": "notebooks/COBA.html",
    "title": "ANNarchy tutorial",
    "section": "",
    "text": "!pip install ANNarchy\n\nThis script reproduces the benchmark used in:\n\nBrette, R., Rudolph, M., Carnevale, T., Hines, M., Beeman, D., Bower, J. M., et al. (2007), Simulation of networks of spiking neurons: a review of tools and strategies., J. Comput. Neurosci., 23, 3, 349–98\n\nbased on the balanced network proposed by:\n\nVogels, T. P. and Abbott, L. F. (2005), Signal propagation and logic gating in networks of integrate-and-fire neurons., J. Neurosci., 25, 46, 10786–95\n\nThe network is composed of 4000 neurons (3200 excitatory and 800 inhibitory), reciprocally connected with a probability of 0.02 (sparse connectivity).\nThe COBA model uses conductance-based IF neurons:\n\\tau \\cdot \\frac{dv (t)}{dt} = E_l - v(t) + g_\\text{exc} (t) \\, (E_\\text{exc} - v(t)) + g_\\text{inh} (t) \\, (E_\\text{inh} - v(t)) + I(t)\nThe discretization step has to be set to 0.1 ms:\n\nfrom ANNarchy import * \nsetup(dt=0.1) \n\nANNarchy 4.7 (4.7.1.5) on darwin (posix).\n\n\n\n\n\nCOBA = Neuron(\n    parameters=\"\"\"\n        El = -60.0          : population\n        Vr = -60.0          : population\n        Erev_exc = 0.0      : population\n        Erev_inh = -80.0    : population\n        Vt = -50.0          : population\n        tau = 20.0          : population\n        tau_exc = 5.0       : population\n        tau_inh = 10.0      : population\n        I = 20.0            : population\n    \"\"\",\n    equations=\"\"\"\n        tau * dv/dt = (El - v) + g_exc * (Erev_exc - v) + g_inh * (Erev_inh - v ) + I\n\n        tau_exc * dg_exc/dt = - g_exc\n        tau_inh * dg_inh/dt = - g_inh\n    \"\"\",\n    spike = \"v > Vt\",\n    reset = \"v = Vr\",\n    refractory = 5.0\n)\n\nThe neuron defines exponentially-decreasing conductance g_exc and g_inh for the excitatory and inhibitory conductances, respectively.\nIt also defines a refractory period of 5 ms.\n\n\n\n\nP = Population(geometry=4000, neuron=COBA)\nPe = P[:3200]\nPi = P[3200:]\n\nWe create a population of 4000 COBA neurons, and assign the 3200 first ones to the excitatory population and the 800 last ones to the inhibitory population.\nIt would have been equivalent to declare two separate populations as:\nPe = Population(geometry=3200, neuron=COBA)\nPi = Population(geometry= 800, neuron=COBA)\nbut splitting a global population allows to apply methods to all neurons, for example when recording all spikes with a single monitor, or when initializing populations parameters uniformly:\n\nP.v = Normal(-55.0, 5.0)\nP.g_exc = Normal(4.0, 1.5)\nP.g_inh = Normal(20.0, 12.0)\n\n\n\n\nThe neurons are randomly connected with a probability of 0.02. Excitatory neurons project on all other neurons with the target “exc” and a weight of 0.6, while the inhibitory neurons have the target “inh” and a weight of 6.7.\n\nCe = Projection(pre=Pe, post=P, target='exc')\nCe.connect_fixed_probability(weights=0.6, probability=0.02)\n\nCi = Projection(pre=Pi, post=P, target='inh')\nCi.connect_fixed_probability(weights=6.7, probability=0.02)\n\n<ANNarchy.core.Projection.Projection at 0x117af1e20>\n\n\n\ncompile()\n\n\n\n\nWe first define a monitor to record the spikes emitted in the whole population:\n\nm = Monitor(P, ['spike'])\n\nWe can then simulate for 100 millisecond:\n\nsimulate(100.)\n\nWe retrieve the recorded spikes from the monitor:\n\ndata = m.get('spike')\n\nand compute a raster plot from the data:\n\nt, n = m.raster_plot(data)\n\nt and n are lists representing for each spike emitted during the simulation the time at which it was emitted and the index the neuron which fired. The length of this list represents the total number of spikes in the population, so we can compute the population mean firing rate:\n\nprint('Mean firing rate in the population: ' + str(len(t) / 4000.) + 'Hz')\n\nMean firing rate in the population: 1.9125Hz\n\n\nFinally, we can show the raster plot with pylab:\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nsns.set_context(\"talk\")\n\nplt.figure(figsize=(10, 8))\nplt.plot(t, n, '.')\nplt.xlabel('Time (ms)')\nplt.ylabel('# neuron')\n\n\nsns.despine()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWe can also plot the mean firing rate in the population over time:\n\nrate = m.population_rate(data)\n\nplt.figure(figsize=(10, 6))\nplt.plot(np.linspace(0, 100, 1001), rate)\nplt.xlabel('Time (ms)')\nplt.ylabel('Population firing rate')\n\nsns.despine()\nplt.tight_layout()"
  },
  {
    "objectID": "notebooks/Content.html",
    "href": "notebooks/Content.html",
    "title": "Notebooks",
    "section": "",
    "text": "Download the Jupyter notebook on github: RC.ipynb\nRun it directly on colab: RC.ipynb"
  },
  {
    "objectID": "notebooks/Content.html#bcm-learning-rule",
    "href": "notebooks/Content.html#bcm-learning-rule",
    "title": "Notebooks",
    "section": "BCM learning rule",
    "text": "BCM learning rule\nDownload the Jupyter notebook on github: RC.ipynb\nRun it directly on colab: RC.ipynb"
  },
  {
    "objectID": "notebooks/Miconi.html",
    "href": "notebooks/Miconi.html",
    "title": "ANNarchy tutorial",
    "section": "",
    "text": "Reward-modulated recurrent network based on:\n\nMiconi T. (2017). Biologically plausible learning in recurrent neural networks reproduces neural dynamics observed during cognitive tasks. eLife 6:e20899. doi:10.7554/eLife.20899\n\n\n!pip install ANNarchy\n\n\nfrom ANNarchy import *\nclear()\nsetup(dt=1.0)\n\nANNarchy 4.7 (4.7.1.5) on darwin (posix).\n\n\n\n\n\n\n\nEach neuron in the reservoir follows the following equations:\n\n    \\tau \\frac{dx(t)}{dt} + x(t) = \\sum_\\text{input} W^\\text{IN} \\, r^\\text{IN}(t) + \\sum_\\text{rec} W^\\text{REC} \\, r(t) + \\xi(t)\n\n\n    r(t) = \\tanh(x(t))\n\nwhere \\xi(t) is a random perturbation at 3 Hz, with an amplitude randomly sampled between -A and +A.\nWe additionally keep track of the mean firing rate with a sliding average:\n\n    \\tilde{x}(t) = \\alpha \\, \\tilde{x}(t) + (1 - \\alpha) \\, x(t)\n\nSome neurons keep a constant rate throughout learning (1 or -1) to provide some bias to the other neurons.\n\nneuron = Neuron(\n    parameters = \"\"\"\n        tau = 30.0 : population # Time constant\n        constant = 0.0 # The four first neurons have constant rates\n        alpha = 0.05 : population # To compute the sliding mean\n        f = 3.0 : population # Frequency of the perturbation\n        A = 16. : population # Perturbation amplitude. dt*A/tau should be 0.5...\n    \"\"\",\n    equations=\"\"\"\n        # Perturbation\n        perturbation = if Uniform(0.0, 1.0) < f/1000.: 1.0 else: 0.0 \n        noise = if perturbation > 0.5: A*Uniform(-1.0, 1.0) else: 0.0\n\n        # ODE for x\n        x += dt*(sum(in) + sum(exc) - x + noise)/tau\n\n        # Output r\n        rprev = r\n        r = if constant == 0.0: tanh(x) else: tanh(constant)\n\n        # Sliding mean\n        delta_x = x - x_mean\n        x_mean = alpha * x_mean + (1 - alpha) * x\n    \"\"\"\n)\n\nThe learning rule is defined by a trace e_{i, j}(t) for each synapse i \\rightarrow j incremented at each time step with:\n\n    e_{i, j}(t) = e_{i, j}(t-1) + (r_i (t) \\, x_j(t))^3\n\nAt the end T of a trial, a normalized reward (R -R_\\text{mean}) is delivered and all weights are updated using:\n\n    \\Delta w_{i, j} = - \\eta \\,  e_{i, j}(T) \\, (R -R_\\text{mean})\n\nAll traces are then reset to 0 for the next trial. Weight changes are clamped between -0.0003 and 0.0003.\nAs ANNarchy applies the synaptic equations at each time step, we need to introduce a boolean learning_phase which performs trace integration when 0, weight update when 1.\n\nsynapse = Synapse(\n    parameters=\"\"\"\n        eta = 0.5 : projection # Learning rate\n        learning_phase = 0.0 : projection # Flag to allow learning only at the end of a trial\n        error = 0.0 : projection # Reward received\n        mean_error = 0.0 : projection # Mean Reward received\n        max_weight_change = 0.0003 : projection # Clip the weight changes\n    \"\"\",\n    equations=\"\"\"\n        # Trace\n        trace += if learning_phase < 0.5:\n                    power(pre.rprev * (post.delta_x), 3)\n                 else:\n                    0.0\n\n        # Weight update only at the end of the trial\n        delta_w = if learning_phase > 0.5:\n                eta * trace * (mean_error) * (error - mean_error)\n             else:\n                 0.0 : min=-max_weight_change, max=max_weight_change\n        w -= if learning_phase > 0.5:\n                delta_w\n             else:\n                 0.0\n    \"\"\"\n)\n\nWe model the DNMS task of Miconi. The RC network has two inputs A and B. The reservoir has 200 neurons, 3 of which have constant rates.\n\n# Input population\ninp = Population(2, Neuron(parameters=\"r=0.0\"))\n\n# Recurrent population\nN = 200\npop = Population(N, neuron)\npop[1].constant = 1.0\npop[10].constant = 1.0\npop[11].constant = -1.0\npop.x = Uniform(-0.1, 0.1)\n\nInput weights are uniformly distributes between -1 and 1.\nRecurrent weights and normally distributed, with a coupling strength of g=1.5 (edge of chaos).\nConnections are all-to-all (fully connected).\n\n# Input weights\nWi = Projection(inp, pop, 'in')\nWi.connect_all_to_all(weights=Uniform(-1.0, 1.0))\n\n# Recurrent weights\ng = 1.5\nWrec = Projection(pop, pop, 'exc', synapse)\nWrec.connect_all_to_all(weights=Normal(0., g/np.sqrt(N)), allow_self_connections=True)\n\n<ANNarchy.core.Projection.Projection at 0x10fc80850>\n\n\n\ncompile()\n\nCompiling ...  OK \n\n\nThe output of the reservoir is chosen to be the neuron of index 100.\n\noutput_neuron = 100\n\nWe record the rates inside the reservoir:\n\nm = Monitor(pop, ['r'])\n\nParameters defining the task:\n\n# Compute the mean reward per trial\nR_mean = np.zeros((2, 2))\nalpha = 0.75 # 0.33\n\n# Durations\nd_stim = 200\nd_delay= 200\nd_response = 400\nd_execution= 200\n\nDefinition of a DNMS trial (AA, AB, BA, BB):\n\ndef dnms_trial(trial, first, second, R_mean):\n    traces = []\n\n    # Reinitialize network\n    pop.x = Uniform(-0.1, 0.1).get_values(N)\n    pop.r = np.tanh(pop.x)\n    pop[1].r = np.tanh(1.0)\n    pop[10].r = np.tanh(1.0)\n    pop[11].r = np.tanh(-1.0)\n\n    # First input\n    inp[first].r = 1.0\n    simulate(d_stim)\n    \n    # Delay\n    inp.r = 0.0\n    simulate(d_delay)\n    \n    # Second input\n    inp[second].r = 1.0\n    simulate(d_stim)\n    \n    # Relaxation\n    inp.r = 0.0\n    simulate(d_response)\n    \n    # Read the output\n    rec = m.get()\n    \n    # Compute the target\n    target = 0.98 if first != second else -0.98\n    \n    # Response is over the last 200 ms\n    output = rec['r'][-int(d_execution):, output_neuron] # neuron 100 over the last 200 ms\n    \n    # Compute the error\n    error = np.mean(np.abs(target - output))\n    print('Target:', target, '\\tOutput:', \"%0.3f\" % np.mean(output), '\\tError:',  \"%0.3f\" % error, '\\tMean:', \"%0.3f\" % R_mean[first, second])\n    \n    # The first 25 trial do not learn, to let R_mean get realistic values\n    if trial > 25:\n        # Apply the learning rule\n        Wrec.learning_phase = 1.0\n        Wrec.error = error\n        Wrec.mean_error = R_mean[first, second]\n        # Learn for one step\n        step()\n        # Reset the traces\n        Wrec.learning_phase = 0.0\n        Wrec.trace = 0.0\n        _ = m.get() # to flush the recording of the last step\n\n    # Update the mean reward\n    R_mean[first, second] = alpha * R_mean[first, second] + (1.- alpha) * error\n\n    return rec, traces, R_mean\n\n\n# Initial weights\ninit_w = Wrec.w\n\n# Many trials of each type\ntry:\n    for trial in range(2500):\n        print('Trial', trial)\n        recordsAA, tracesAA, R_mean = dnms_trial (trial, 0, 0, R_mean)\n        recordsAB, tracesAB, R_mean = dnms_trial (trial, 0, 1, R_mean)\n        recordsBA, tracesBA, R_mean = dnms_trial (trial, 1, 0, R_mean)\n        recordsBB, tracesBB, R_mean = dnms_trial (trial, 1, 1, R_mean)\n        if trial == 0:\n            initialAA = recordsAA['r']\n            initialAB = recordsAB['r']\n            initialBA = recordsBA['r']\n            initialBB = recordsBB['r']\nexcept KeyboardInterrupt:\n    pass\n\n# Final weights\nfinal_w = Wrec.w\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nsns.set_context(\"talk\")\n\nplt.figure(figsize=(10, 10))\n\nax = plt.subplot(221)\nax.plot(np.mean(initialAA[:, output_neuron:output_neuron+1], axis=1), label='before')\nax.plot(np.mean(recordsAA['r'][:, output_neuron:output_neuron+1], axis=1), label='after')\nax.set_ylim((-1., 1.))\nax.legend()\nax.set_title('Output AA -1')\nax = plt.subplot(222)\nax.plot(np.mean(initialBA[:, output_neuron:output_neuron+1], axis=1), label='before')\nax.plot(np.mean(recordsBA['r'][:, output_neuron:output_neuron+1], axis=1), label='after')\nax.set_ylim((-1., 1.))\nax.legend()\nax.set_title('Output BA +1')\nax = plt.subplot(223)\nax.plot(np.mean(initialAB[:, output_neuron:output_neuron+1], axis=1), label='before')\nax.plot(np.mean(recordsAB['r'][:, output_neuron:output_neuron+1], axis=1), label='after')\nax.set_ylim((-1., 1.))\nax.set_title('Output AB +1')\nax = plt.subplot(224)\nax.plot(np.mean(initialBB[:, output_neuron:output_neuron+1], axis=1), label='before')\nax.plot(np.mean(recordsBB['r'][:, output_neuron:output_neuron+1], axis=1), label='after')\nax.set_ylim((-1., 1.))\nax.set_title('Output BB -1')\nplt.show()"
  },
  {
    "objectID": "notebooks/AdEx.html",
    "href": "notebooks/AdEx.html",
    "title": "ANNarchy tutorial",
    "section": "",
    "text": "This notebook explores how the AdEx neuron model can reproduce various spiking patterns observed in vivo.\nCode based on:\n\nNaud, R., Marcille, N., Clopath, C., and Gerstner, W. (2008). Firing patterns in the adaptive exponential integrate-and-fire model. Biol Cybern 99, 335. doi:10.1007/s00422-008-0264-7.\n\nOn colab:\n\n!pip install ANNarchy\n\n\nfrom ANNarchy import *\nclear()\nsetup(dt=0.1)\n\nANNarchy 4.7 (4.7.1.5) on darwin (posix).\n\n\nThe AdEx neuron is defined by the following equations:\n\n    C \\, \\frac{dv}{dt} = -g_L \\ (v - E_L) + g_L \\, \\Delta_T \\, \\exp(\\frac{v - v_T}{\\Delta_T}) + I - w\n\n\n    \\tau_w \\, \\frac{dw}{dt} = a \\, (v - E_L) - w\n\nif v > v_\\text{spike}:\n\nv = v_R\nw = w + b\n\n\nAdEx = Neuron(\n    parameters=\"\"\"\n        C = 200.\n        gL = 10. # not g_L! g_ is reserved for spike transmission\n        E_L = -70.\n        v_T = -50.\n        delta_T = 2.0\n        a = 2.0\n        tau_w = 30.\n        b = 0.\n        v_r = -58.\n        I = 500.\n        v_spike = 0.0 \n    \"\"\",\n    equations=\"\"\"\n        C * dv/dt = - gL * (v - E_L) +  gL * delta_T * exp((v-v_T)/delta_T) + I - w : init=-70.0     \n        tau_w * dw/dt = a * (v - E_L) - w  : init=0.0\n    \"\"\",\n    spike=\"\"\"\n        v >= v_spike\n    \"\"\",\n    reset=\"\"\"\n        v = v_r\n        w += b\n    \"\"\",\n    refractory = 2.0\n)\n\nWe create a population of 8 AdEx neurons which will get different parameter values.\n\npop = Population(8, AdEx)\n\n\ncompile()\n\nWe add a monitor to track the membrane potential and the spike timings during the simulation.\n\nm = Monitor(pop, ['v', 'spike'])\n\nAs in the paper, we provide different parameters to each neuron and simulate the network for 500 ms with a fixed input current, and remove that current for an additional 50 ms.\n\n# a) tonic spiking b) adaptation, c) initial burst, d) regular bursting, e) delayed accelerating, f) delayed regular bursting, g) transcient spiking, h) irregular spiking\npop.C =       [200, 200, 130, 200, 200, 200, 100, 100]\npop.gL =      [ 10,  12,  18,  10,  12,  12,  10,  12]\npop.E_L =     [-70, -70, -58, -58, -70, -70, -65, -60]\npop.v_T =     [-50, -50, -50, -50, -50, -50, -50, -50]\npop.delta_T = [  2,   2,   2,   2,   2,   2,   2,   2]\npop.a =       [  2,   2,   4,   2,-10., -6.,-10.,-11.]\npop.tau_w =   [ 30, 300, 150, 120, 300, 300,  90, 130]\npop.b =       [  0,  60, 120, 100,   0,   0,  30,  30]\npop.v_r =     [-58, -58, -50, -46, -58, -58, -47, -48]\npop.I =       [500, 500, 400, 210, 300, 110, 350, 160]\n\n# Reset neuron\npop.v = pop.E_L\npop.w = 0.0\n\n# Simulate\nsimulate(500.)\npop.I = 0.0\nsimulate(50.)\n\n# Recordings\ndata = m.get('v')\nspikes = m.get('spike')\nfor n, t in spikes.items(): # Normalize the spikes\n    data[[x - m.times()['v']['start'][0] for x in t], n] = 0.0\n\nWe can now visualize the simulations:\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nsns.set_context(\"talk\")\n\ntitles = [\n    \"a) tonic spiking\", \n    \"b) adaptation\", \n    \"c) initial burst\", \n    \"d) regular bursting\", \n    \"e) delayed accelerating\", \n    \"f) delayed regular bursting\", \n    \"g) transcient spiking\", \n    \"h) irregular spiking\"\n]\n\nplt.figure(figsize=(20, 20))\nplt.ylim((-70., 0.))\nfor i in range(8):\n    plt.subplot(4, 2, i+1)\n    plt.title(titles[i])\n    plt.plot(data[:, i], lw=3)\n    \n\nsns.despine()\nplt.tight_layout()\nplt.show()\n\n/var/folders/6w/6msx49ws7k13cc0bbys0tt4m0000gn/T/ipykernel_72075/1457585605.py:21: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\n  plt.subplot(4, 2, i+1)"
  },
  {
    "objectID": "notebooks/SynapticTransmission.html",
    "href": "notebooks/SynapticTransmission.html",
    "title": "ANNarchy tutorial",
    "section": "",
    "text": "This notebook simply demonstrates the three main type of synaptic transmission for spiking neurons:\n\nInstantaneous\nExponentially-decreasing\nAlpha-shaped\n\n\n!pip install ANnarchy\n\n\nfrom ANNarchy import *\nclear()\n\nANNarchy 4.7 (4.7.1.5) on darwin (posix).\n\n\nWe use here a simple LIF neuron receving three types of projections (a, b, c). The conductance g_a uses instantaneous transmission, as it is reset to 0 after each step. g_b decreases exponentially with time following a first order ODE. g_c is integrated twice in alpha_c, leading to the alpha shape.\nAll methods use the exponential numerical method, as they are first order linear ODEs and can be solved exactly.\n\nLIF = Neuron(\n    parameters=\"\"\"\n        tau = 20.\n        E_L = -70.\n        v_T = 0.\n        v_r = -58.\n        tau_b = 10.0\n        tau_c = 10.0\n    \"\"\",\n    equations=\"\"\"\n        # Membrane potential\n        tau * dv/dt = (E_L - v) + g_a + g_b + alpha_c : init=-70.\n        \n        # Exponentially decreasing\n        tau_b * dg_b/dt = -g_b : exponential\n        \n        # Alpha-shaped\n        tau_c * dg_c/dt = -g_c : exponential\n        tau_c * dalpha_c/dt = exp((tau_c - dt/2.0)/tau_c) * g_c - alpha_c  : exponential\n    \"\"\",\n    spike=\"v >= v_T\",\n    reset=\"v = v_r\",\n    refractory = 2.0\n)\n\nThe LIF neuron will receive a single spike at t = 10ms, using the SpikeSourceArray specific population.\n\ninp = SpikeSourceArray([10.])\npop = Population(1, LIF)\n\nWe implement three different projections between the same neurons, to highlight the three possible transmission mechanisms.\n\nproj = Projection(inp, pop, 'a').connect_all_to_all(weights=1.0)\nproj = Projection(inp, pop, 'b').connect_all_to_all(weights=1.0)\nproj = Projection(inp, pop, 'c').connect_all_to_all(weights=1.0)\n\n\ncompile()\n\nCompiling ...  OK \n\n\nWe monitor the three conductances:\n\nm = Monitor(pop, ['g_a', 'g_b', 'alpha_c'])\n\n\ninp.clear()\nsimulate(100.)\n\n\ndata = m.get()\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nsns.set_context(\"talk\")\n\nplt.figure(figsize=(10, 10))\nplt.subplot(311)\nplt.plot(data['g_a'][:, 0])\nplt.ylabel(\"Instantaneous\")\nplt.subplot(312)\nplt.plot(data['g_b'][:, 0])\nplt.ylabel(\"Exponential\")\nplt.subplot(313)\nplt.plot(data['alpha_c'][:, 0])\nplt.xlabel(\"Time (ms)\")\nplt.ylabel(\"Alpha\")\nplt.show()"
  },
  {
    "objectID": "notebooks/BCM.html",
    "href": "notebooks/BCM.html",
    "title": "ANNarchy tutorial",
    "section": "",
    "text": "The goal of this notebook is to investigate the Intrator & Cooper BCM learning rule for rate-coded networks.\n\\Delta w = \\eta \\, r^\\text{pre} \\, r^\\text{post}  \\,  (r^\\text{post} - \\mathbb{E}[(r^\\text{post})^2])\n\nIntrator, N., & Cooper, L. N. (1992). Objective function formulation of the BCM theory of visual cortical plasticity: Statistical connections, stability conditions. Neural Networks, 5(1), 3–17. https://doi.org/10.1016/S0893-6080(05)80003-6\n\n\n!pip install ANNarchy\n\nWe can now import ANNarchy:\n\nfrom ANNarchy import *\nclear()\nsetup(dt=1.0)\n\nANNarchy 4.7 (4.7.1.5) on darwin (posix).\n\n\nWe will keep a minimal experimental setup, with two input neurons connected to a single output neuron. Note how the input neurons are defined by setting r as a parameter that can be set externally.\n\n# Input\ninput_neuron = Neuron(\n    parameters = \"\"\"\n        r = 0.0\n    \"\"\"\n)\npre = Population(2, input_neuron)\n\n# Output\nneuron = Neuron(\n    equations = \"\"\"\n        r = sum(exc)\n    \"\"\"\n)\npost = Population(1, neuron)\n\nWe can now define a synapse model implementing the Intrator and Cooper version of the BCM learning rule.\nThe synapse has two parameters: The learning rate eta and the time constant tau of the moving average theta. Both are defined as projection parameters, as we only need one value for the whole projection. If you omit this flag, there will be one value per synapse, which would be a waste of RAM.\nThe moving average theta tracks the square of the post-synaptic firing rate post.r. It has the flag postsynaptic, as we need to compute only one variable per post-synaptic neuron (it does not really matter in our example as have only one output neuron…). It uses the exponential numerical method, as it is a first-order linear ODE that can be solved exactly. However, the default explicit Euler method would work just as well here.\nThe weight change dw/dt follows the BCM learning rule. min=0.0 ensures that the weight w stays positive throughout learning. The explicit Euler method is the default and could be omitted.\nThe psp argument w * pre.r (what is summed by the post-synaptic neuron over its incoming connections) is also the default value and could be omitted.\n\nIBCM = Synapse(\n    parameters = \"\"\"\n        eta = 0.01 : projection\n        tau = 100.0 : projection\n    \"\"\",\n    equations = \"\"\"\n        tau * dtheta/dt + theta = (post.r)^2 : postsynaptic, exponential\n\n        dw/dt = eta * post.r * (post.r - theta) * pre.r : min=0.0, explicit\n    \"\"\",\n    psp = \"w * pre.r\"\n)\n\nWe can now create a projection between the two populations using the synapse type. The connection method is all-to-all, initialozing the two weights to 1.\n\nproj = Projection(pre, post, 'exc', IBCM)\nproj.connect_all_to_all(1.0)\n\n<ANNarchy.core.Projection.Projection at 0x1198dea00>\n\n\nWe can now compile the network and record the post-synaptic firing rate as well as the evolution of the weights and thresholds during learning.\n\ncompile()\n\nm = Monitor(post, 'r')\nn = Monitor(proj, ['w', 'theta'])\n\nCompiling ...  OK \nWARNING: Monitor(): it is a bad idea to record synaptic variables of a projection at each time step! \n\n\nThe simulation protocol is kept simple, as it consists of setting constant firing rates for the two input neurons and simulating for one second.\n\npre.r = np.array([1.0, 0.1])\nsimulate(1000.)\n\nWe can now retrieve the recordings and plot the evolution of the various variables.\n\nr = m.get('r')\nw = n.get('w')\ntheta = n.get('theta')\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nsns.set_context(\"talk\")\n\nplt.figure(figsize=(10, 5))\nplt.subplot(211)\nplt.plot(r[:, 0], label='r')\nplt.plot(theta[:, 0], label='theta')\nplt.legend()\nplt.subplot(212)\nplt.plot(w[:, 0, 0], label=\"$w_1$\")\nplt.plot(w[:, 0, 1], label=\"$w_2$\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nNotice how the first weight increases when r is higher than theta (LTP), but decreases afterwards (LTD). Unintuitively, the input neuron with the highest activity sees its weight decreased at the end of the stimulation."
  },
  {
    "objectID": "notebooks/STDP.html",
    "href": "notebooks/STDP.html",
    "title": "ANNarchy tutorial",
    "section": "",
    "text": "!pip install ANNarchy\n\n\nfrom ANNarchy import *\nclear()\nsetup(dt=1.0)\n\nANNarchy 4.7 (4.7.1.5) on darwin (posix).\n\n\nThe STDP learning rule maintains exponentially-decaying traces for the pre-synaptic and post-synaptic spikes.\n\\tau^+ \\, \\frac{d x(t)}{dt} = -x (t)\n\\tau^- \\, \\frac{d y(t)}{dt} = -x (t)\nLTP and LTD occur at spike times depending on the corresponding traces.\n\nWhen a pre-synaptic spike occurs, x(t) is incremented and LTD is applied proportionally to y(t).\nWhen a post-synaptic spike occurs, y(t) is incremented and LTP is applied proportionally to x(t).\n\n\nSTDP = Synapse(\n    parameters = \"\"\"\n        tau_plus = 20.0 : projection ; tau_minus = 20.0 : projection\n        A_plus = 0.01 : projection   ; A_minus = 0.01 : projection\n        w_min = 0.0 : projection     ; w_max = 2.0 : projection\n    \"\"\",\n    equations = \"\"\"\n    \n        tau_plus * dx/dt = -x : event-driven # pre-synaptic trace\n    \n        tau_minus * dy/dt = -y : event-driven # post-synaptic trace\n    \n    \"\"\",\n    pre_spike=\"\"\"\n        \n        g_target += w\n        \n        x += A_plus * w_max\n        \n        w = clip(w - y, w_min , w_max) # LTD\n    \"\"\",\n    post_spike=\"\"\"\n        \n        y += A_minus * w_max\n        \n        w = clip(w + x, w_min , w_max) # LTP\n    \"\"\"\n)\n\nWe create two dummy populations with one neuron each, whose spike times we can control.\n\npre = SpikeSourceArray([[0.]])\npost = SpikeSourceArray([[50.]])\n\nWe connect the population using a STDP synapse.\n\nproj = Projection(pre, post, 'exc', STDP)\nproj.connect_all_to_all(1.0)\n\n<ANNarchy.core.Projection.Projection at 0x1170b24f0>\n\n\n\ncompile()\n\nCompiling ...  OK \n\n\nThe presynaptic neuron will fire at avrious times between 0 and 100 ms, while the postsynaptic neuron keeps firing at 50 ms.\n\npre_times = np.linspace(100.0, 0.0, 101)\n\n\nweight_changes = []\nfor t_pre in pre_times:\n    \n    # Reset the populations\n    pre.clear()\n    post.clear()\n    pre.spike_times = [[t_pre]]\n    post.spike_times = [[50.0]]\n    \n    # Reset the traces\n    proj.x = 0.0\n    proj.y = 0.0\n    \n    # Weight before the simulation\n    w_before = proj[0].w[0]\n    \n    # Simulate long enough\n    simulate(105.0)\n    \n    # Record weight change\n    delta_w = proj[0].w[0] - w_before\n    weight_changes.append(delta_w)\n\nWe can now plot the classical STDP figure:\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nsns.set_context(\"talk\")\n\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(1, 1, 1)\nplt.plot(50. - pre_times, weight_changes, \"*\")\nplt.xlabel(\"t_post - t_pre\")\nplt.ylabel(\"delta_w\")\n\nsns.despine()\n\nax.spines['left'].set_position('zero')\nax.spines['bottom'].set_position('zero')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/STP.html",
    "href": "notebooks/STP.html",
    "title": "ANNarchy tutorial",
    "section": "",
    "text": "Implementation of the recurrent network proposed in:\n\nTsodyks, Uziel and Markram (2000). Synchrony Generation in Recurrent Networks with Frequency-Dependent Synapses, The Journal of Neuroscience, 20(50).\n\n\nfrom ANNarchy import *\nclear()\ndt=0.25\nsetup(dt=dt)\n\nANNarchy 4.7 (4.7.1.5) on darwin (posix).\n\n\nThis network uses simple leaky integrate-and-fire (LIF) neurons. The network is compsed of 400 excitatory and 100 inhibitory neurons, receiving increasingly strong input currents.\n\nLIF = Neuron(\n    parameters = \"\"\"\n    tau = 30.0 : population\n    I = 15.0\n    tau_I = 3.0 : population\n    \"\"\",\n    equations = \"\"\"\n    tau * dv/dt = -v + g_exc - g_inh + I : init=13.5\n    tau_I * dg_exc/dt = -g_exc\n    tau_I * dg_inh/dt = -g_inh\n    \"\"\",\n    spike = \"v > 15.0\",\n    reset = \"v = 13.5\",\n    refractory = 3.0\n)\n\nP = Population(geometry=500, neuron=LIF)\n\nP.I = np.sort(Uniform(14.625, 15.375).get_values(500))\nP.v = Uniform(0.0, 15.0)\n\nExc = P[:400]\nInh = P[400:]\n\nShort-term plasticity can be defined by dynamical changes of synaptic efficiency, based on pre- or post-synaptic activity.\nWe define a STP synapse, whose post-pynaptic potential (psp, define by g_target) depends not only on the weight w and the emission of pre-synaptic spike, but also on intra-synaptic variables x and u:\n\nSTP = Synapse(\n    parameters = \"\"\"\n    w=0.0\n    tau_rec = 1.0\n    tau_facil = 1.0\n    U = 0.1\n    \"\"\",\n    equations = \"\"\"\n    dx/dt = (1 - x)/tau_rec : init = 1.0, event-driven\n    du/dt = (U - u)/tau_facil : init = 0.1, event-driven   \n    \"\"\",\n    pre_spike=\"\"\"\n    g_target += w * u * x\n    x *= (1 - u)\n    u += U * (1 - u)\n    \"\"\"\n)\n\nCreating the projection between the excitatory and inhibitory is straightforward when the right parameters are chosen:\n\n# Parameters for the synapses\nAee = 1.8\nAei = 5.4\nAie = 7.2\nAii = 7.2\n\nUee = 0.5\nUei = 0.5\nUie = 0.04\nUii = 0.04\n\ntau_rec_ee = 800.0\ntau_rec_ei = 800.0\ntau_rec_ie = 100.0\ntau_rec_ii = 100.0\n\ntau_facil_ie = 1000.0\ntau_facil_ii = 1000.0\n\n# Create projections\nproj_ee = Projection(pre=Exc, post=Exc, target='exc', synapse=STP)\nproj_ee.connect_fixed_probability(probability=0.1, weights=Normal(Aee, (Aee/2.0), min=0.2*Aee, max=2.0*Aee)) \nproj_ee.U = Normal(Uee, (Uee/2.0), min=0.1, max=0.9)\nproj_ee.tau_rec = Normal(tau_rec_ee, (tau_rec_ee/2.0), min=5.0)\nproj_ee.tau_facil = dt # Cannot be 0!\n\nproj_ei = Projection(pre=Inh, post=Exc, target='inh', synapse=STP)\nproj_ei.connect_fixed_probability(probability=0.1, weights=Normal(Aei, (Aei/2.0), min=0.2*Aei, max=2.0*Aei))\nproj_ei.U = Normal(Uei, (Uei/2.0), min=0.1, max=0.9)\nproj_ei.tau_rec = Normal(tau_rec_ei, (tau_rec_ei/2.0), min=5.0)\nproj_ei.tau_facil = dt # Cannot be 0!\n\nproj_ie = Projection(pre=Exc, post=Inh, target='exc', synapse=STP)\nproj_ie.connect_fixed_probability(probability=0.1, weights=Normal(Aie, (Aie/2.0), min=0.2*Aie, max=2.0*Aie))\nproj_ie.U = Normal(Uie, (Uie/2.0), min=0.001, max=0.07)\nproj_ie.tau_rec = Normal(tau_rec_ie, (tau_rec_ie/2.0), min=5.0)\nproj_ie.tau_facil = Normal(tau_facil_ie, (tau_facil_ie/2.0), min=5.0)\n\nproj_ii = Projection(pre=Inh, post=Inh, target='inh', synapse=STP)\nproj_ii.connect_fixed_probability(probability=0.1, weights=Normal(Aii, (Aii/2.0), min=0.2*Aii, max=2.0*Aii))\nproj_ii.U = Normal(Uii, (Uii/2.0), min=0.001, max=0.07)\nproj_ii.tau_rec = Normal(tau_rec_ii, (tau_rec_ii/2.0), min=5.0)\nproj_ii.tau_facil = Normal(tau_facil_ii, (tau_facil_ii/2.0), min=5.0)\n\nWe compile and simulate for 10 seconds:\n\ncompile()\n\n\nMe = Monitor(Exc, 'spike')\nMi = Monitor(Inh, 'spike')\n\n\nduration = 10000.0\nsimulate(duration, measure_time=True)\n\nSimulating 10.0 seconds of the network took 0.10498785972595215 seconds. \n\n\nWe retrieve the recordings and plot them:\n\n# Retrieve recordings\ndata_exc = Me.get()\ndata_inh = Mi.get()\nte, ne = Me.raster_plot(data_exc['spike'])\nti, ni = Mi.raster_plot(data_inh['spike'])\n\n# Histogram of the exc population\nh = Me.histogram(data_exc['spike'], bins=1.0)\n\n# Mean firing rate of each excitatory neuron\nrates = []\nfor neur in data_exc['spike'].keys():\n    rates.append(len(data_exc['spike'][neur])/duration*1000.0)\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nsns.set_context(\"talk\")\n\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nplt.plot(te, ne, 'b.', markersize=1.0)\nplt.plot(ti, ni, 'b.', markersize=1.0)\nplt.xlim((0, duration)); plt.ylim((0,500))\nplt.xlabel('Time (ms)')\nplt.ylabel('# neuron')\n\nplt.subplot(3,1,2)\nplt.plot(h/400.)\nplt.xlabel('Time (ms)')\nplt.ylabel('Net activity')\n\nplt.subplot(3,1,3)\nplt.plot(sorted(rates))\nplt.ylabel('Spikes / sec')\nplt.xlabel('# neuron')\nplt.show()"
  }
]